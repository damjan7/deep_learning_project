{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To run our code for the DL project in google colab, we load all scripts through cells "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as tr\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, dataset, augmentations=None):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = tr.Compose([tr.ToTensor()])\n",
    "\n",
    "        if dataset == \"MNIST\":\n",
    "            self.trainset = MNIST(root=\"data\", train=True, download=True, transform=self.transforms)\n",
    "            self.testset = MNIST(root=\"data\", train=False, download=True, transform=self.transforms)\n",
    "\n",
    "        elif dataset == \"FashionMNIST\":\n",
    "            self.trainset = FashionMNIST(root=\"data\", train=True, download=True, transform=self.transforms)\n",
    "            self.testset = FashionMNIST(root=\"data\", train=False, download=True, transform=self.transforms)\n",
    "\n",
    "\n",
    "        x_train = self.trainset.data.reshape(-1, 28*28)/255.\n",
    "        y_train = self.trainset.targets\n",
    "        self.train_data = TensorDataset(x_train, y_train)\n",
    "\n",
    "        x_test = self.testset.data.reshape(-1, 28*28)/255.\n",
    "        y_test = self.testset.targets\n",
    "        self.test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "        if augmentations is not None:\n",
    "            # increase the size of the training set by applying augmentations\n",
    "            self.transforms_extra = tr.Compose([self.transforms, augmentations])\n",
    "\n",
    "            if dataset == \"MNIST\":\n",
    "                self.trainset_extra = MNIST(root=\"data\", train=True, download=True, transform=self.transforms_extra)\n",
    "            elif dataset == \"FashionMNIST\":\n",
    "                self.trainset_extra = FashionMNIST(root=\"data\", train=True, download=True, transform=self.transforms_extra)\n",
    "\n",
    "            x_train_extra = self.trainset.data.reshape(-1, 28*28)/255.\n",
    "            y_train_extra = self.trainset.targets\n",
    "            train_data_extra = TensorDataset(x_train_extra, y_train_extra)\n",
    "\n",
    "            self.train_data = ConcatDataset([self.train_data, train_data_extra])\n",
    "\n",
    "\n",
    "    def get_data(self, num_train_samples=None):\n",
    "        if num_train_samples is not None:\n",
    "            sub_train_idx = np.random.choice(self.train_data.__len__(), num_train_samples, replace=False)\n",
    "            sub_train_data = Subset(self.train_data, sub_train_idx)\n",
    "\n",
    "        else:\n",
    "            sub_train_data = self.train_data\n",
    "\n",
    "        return sub_train_data, self.test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## priors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "from scipy.special import gamma\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# Framework for priors --------------------------------------------------------\n",
    "\n",
    "class Prior:\n",
    "    \"\"\"\n",
    "    This class is a base class for all priors that we use in this project.\n",
    "    It enforces the implementation of the the log_likelihood and sample methods.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def sample(self,n):\n",
    "        pass\n",
    "    def log_likelihood(self,values):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Isotropic Gaussian prior ----------------------------------------------------\n",
    "\n",
    "class Isotropic_Gaussian(Prior):\n",
    "    \"\"\"\n",
    "    Isotropic Gaussian prior with mean (loc) and standard deviation (scale) as parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, loc: float = 0, scale: float = 1.0, Temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        assert scale > 0, \"Scale must be positive\"\n",
    "        self.loc = torch.tensor(loc, dtype=torch.float32)\n",
    "        self.scale = torch.tensor(scale, dtype=torch.float32)\n",
    "        self.Temperature = torch.tensor(Temperature, dtype=torch.float32)\n",
    "        self.name = \"Isotropic_Gaussian\"\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        return dist.Normal(self.loc, self.scale).log_prob(values).sum() / self.Temperature\n",
    "\n",
    "    def sample(self, n):\n",
    "        return dist.Normal(self.loc, self.scale).sample((n,))\n",
    "\n",
    "\n",
    "# Multivariate Gaussian prior -------------------------------------------------\n",
    "\n",
    "\n",
    "# TODO: Implement this\n",
    "\n",
    "class Multivariate_Diagonal_Gaussian(Prior):\n",
    "    \"\"\"\n",
    "    Multivariate diagonal Gaussian distribution,\n",
    "    i.e., assumes all elements to be independent Gaussians\n",
    "    but with different means and standard deviations.\n",
    "    This parameterizes the standard deviation via a parameter rho as\n",
    "    sigma = softplus(rho).\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor, Temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.sigma = torch.log(1 + torch.exp(rho))\n",
    "        self.Temperature = Temperature\n",
    "        self.name = \"Multivariate_Diagonal_Gaussian\"\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement this\n",
    "        return dist.Normal(self.mu, self.sigma).log_prob(values).sum() / self.Temperature\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        # TODO: Implement this\n",
    "        eps = torch.randn_like(self.mu)\n",
    "        return self.mu + self.sigma * eps\n",
    "\n",
    "\n",
    "\n",
    "# Student-t prior -------------------------------------------------------------\n",
    "\n",
    "class StudentT_prior(Prior):\n",
    "    \"\"\"\n",
    "    Student-T Prior with degrees of freedom (df), mean (loc) and scale (scale) as parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: float = 10, loc: float = 0, scale: float = 1.0, Temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.df = torch.tensor(df, dtype=torch.float32)\n",
    "        self.loc = torch.tensor(loc, dtype=torch.float32)\n",
    "        self.scale = torch.tensor(scale, dtype=torch.float32)\n",
    "        self.Temperature = torch.tensor(Temperature, dtype=torch.float32)\n",
    "        self.name = \"Student_T\"\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        return dist.StudentT(self.df, self.loc, self.scale).log_prob(values).sum() / self.Temperature\n",
    "\n",
    "    def sample(self, n):\n",
    "        return dist.StudentT(self.df, self.loc, self.scale).sample((n,))  \n",
    "\n",
    "\n",
    "# Laplace prior ---------------------------------------------------------------\n",
    "\n",
    "class Laplace_prior(Prior):\n",
    "    \"\"\"\n",
    "    Laplace Prior with mean (loc) and scale (scale) as parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, loc: float = 0, scale: float = 1.0, Temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.loc = torch.tensor(loc, dtype=torch.float32)\n",
    "        self.scale = torch.tensor(scale, dtype=torch.float32)\n",
    "        self.Temperature = torch.tensor(Temperature, dtype=torch.float32)\n",
    "        self.name = \"Laplace\"\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        return dist.Laplace(self.loc, self.scale).log_prob(values).sum() / self.Temperature\n",
    "\n",
    "    def sample(self, n) -> torch.Tensor:\n",
    "        return dist.Laplace(self.loc, self.scale).sample((n,))\n",
    "\n",
    "\n",
    "\n",
    "# Gaussian mixture prior ------------------------------------------------------\n",
    "\n",
    "class Gaussian_Mixture(Prior):\n",
    "    \"\"\"\n",
    "    Mixture of two Gaussians with means (loc1, loc2), standard deviations (scale1, scale2) and mixing coefficient (mixing_coef) as parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, loc1: float = 0, scale1: float = 3.0, loc2: float = 0, scale2: float = 1.0,\n",
    "                mixing_coef: float = 0.7, Temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.loc1 = torch.tensor(loc1, dtype=torch.float32)\n",
    "        self.loc2 = torch.tensor(loc2, dtype=torch.float32)\n",
    "        self.scale1 = torch.tensor(scale1, dtype=torch.float32)\n",
    "        self.scale2 = torch.tensor(scale2, dtype=torch.float32)\n",
    "        self.mixing_coef = torch.tensor(mixing_coef, dtype=torch.float32)\n",
    "        self.Temperature = torch.tensor(Temperature, dtype=torch.float32)\n",
    "        self.name = \"Gaussian_Mixture\"\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        p1 = dist.Normal(self.loc1, self.scale1).log_prob(values)\n",
    "        p2 = dist.Normal(self.loc2, self.scale2).log_prob(values)\n",
    "        log_lik = (p1 * self.mixing_coef + p2 * (1-self.mixing_coef)).sum() / self.Temperature\n",
    "        return log_lik\n",
    "\n",
    "    def sample(self, n) -> torch.Tensor:\n",
    "        sample1 = dist.Normal(self.loc1, self.scale1).sample((n,))\n",
    "        sample2 = dist.Normal(self.loc2, self.scale2).sample((n,))\n",
    "        return sample1 * self.mixing_coef + sample2 * (1-self.mixing_coef)\n",
    "\n",
    "\n",
    "# Normal Inverse Gamma prior --------------------------------------------------\n",
    "\n",
    "class Inverse_Gamma(Prior):\n",
    "    \"\"\" \n",
    "    Inverse Gamma distribution with shape (shape) and rate (rate) as parameters.\n",
    "    This distribution is needed for the Normal Inverse Gamma prior.\n",
    "    \"\"\"\n",
    "    def __init__(self, shape: float = 1.0, rate: float = 1.0, Temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.shape = torch.tensor(shape, dtype=torch.float32)\n",
    "        self.rate = torch.tensor(rate, dtype=torch.float32)\n",
    "        self.Temperature = torch.tensor(Temperature, dtype=torch.float32)\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the value of the predictive log likelihood at the target value\n",
    "        \"\"\"\n",
    "        x = (self.rate**self.shape) / gamma(self.shape)\n",
    "        y = values**(-self.shape - 1)\n",
    "        z = torch.exp(-self.rate / values)\n",
    "        return torch.log(x * y * z)\n",
    "\n",
    "    def sample(self, n) -> torch.Tensor:\n",
    "        # sample from gamma and return 1/x\n",
    "        x = dist.Gamma(self.shape, self.rate).sample((n,))\n",
    "        return 1/x\n",
    "\n",
    "\n",
    "class Normal_Inverse_Gamma(Prior):\n",
    "    \"\"\" \n",
    "    Normal Inverse Gamma distribution with mean (mu), precision (lam), shape (alpha) and rate (beta) as parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, loc: float = 0, lam: float = 1, alpha: float = 1, beta: float = 1, Temperature: float = 1.0,device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        loc: loc of the normal distribution\n",
    "        lam: precision of the normal distribution\n",
    "        alpha: shape parameter of the inverse gamma distribution\n",
    "        beta: rate parameter of the inverse gamma distribution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.loc = torch.tensor(loc, dtype=torch.float32,device=self.device)\n",
    "        self.lam = torch.tensor(lam, dtype=torch.float32,device=self.device)\n",
    "        self.alpha = torch.tensor(alpha, dtype=torch.float32,device=self.device)\n",
    "        self.beta = torch.tensor(beta, dtype=torch.float32,device=self.device)\n",
    "        self.Temperature = torch.tensor(Temperature, dtype=torch.float32,device=self.device)\n",
    "        self.name = \"Normal_Inverse_Gamma\"\n",
    "\n",
    "        \n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor, var = torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the likelihood of the inverse gamma distribution for an x and a variance\n",
    "        \"\"\"\n",
    "        # manually compute the likelihood\n",
    "        # var =  self.beta / (self.alpha + 1 + 0.5) + 1e-8 # to avoid division by zero\n",
    "\n",
    "        torch.two = torch.tensor(2,dtype=torch.float32,device = self.device)\n",
    "        torch.pi = torch.acos(torch.zeros(1,device=self.device)).item() * torch.two\n",
    "        # torch.pi.to(self.device)\n",
    "        log_likelihood = torch.xlogy(torch.tensor(0.5,device=self.device), self.lam / (torch.two * torch.pi * var)) + \\\n",
    "                    torch.xlogy(self.alpha,self.beta) - \\\n",
    "                    torch.lgamma(self.alpha) - \\\n",
    "                    torch.xlogy(self.alpha + torch.tensor(1,device=self.device),var) - \\\n",
    "                    torch.div(torch.two*self.beta + self.lam * (values - self.loc)**torch.two,(torch.two * var))\n",
    "        \n",
    "        return log_likelihood/ self.Temperature\n",
    "\n",
    "        #new try:\n",
    "        # for all pos integers, gamma function:\n",
    "        # F(a) = (a-1)!\n",
    "\n",
    "        #var = self.beta / (self.alpha + 1 + 0.5)  # according to formula on wikipedia\n",
    "        #part1 = self.lam ** 0.5 / (2 * np.pi * var) ** 0.5\n",
    "        #part2 = self.beta ** self.alpha / np.math.factorial(self.alpha-1)\n",
    "        #part3 = (1/var) ** (self.alpha + 1)\n",
    "        #part4 = (-2*self.beta - self.lam * (values - self.mu) ** 0.5) / (2*var)\n",
    "        #likelihood = part1 * part2 * part3 * np.exp(part4)\n",
    "        #log_likelihood = np.log(likelihood)\n",
    "        return log_likelihood / self.Temperature\n",
    "\n",
    "\n",
    "    def sample(self, n) -> torch.Tensor:\n",
    "        # sample variance from inverse gamma and sample x from normal given the variance\n",
    "        var = torch.div(1, dist.Gamma(self.alpha, self.beta).sample((1,)))\n",
    "        x = dist.Normal(self.loc, torch.sqrt(var/self.lam)).sample((n,))\n",
    "        return x, var\n",
    "\n",
    "\n",
    "\n",
    "# class Normal_Inverse_Gamma(Prior):\n",
    "#     \"\"\" \n",
    "#     Normal Inverse Gamma distribution with mean (mu), precision (lam), shape (alpha) and rate (beta) as parameters.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, loc: float = 0, lam: float = 1, alpha: float = 1, beta: float = 1, Temperature: float = 1.0):\n",
    "#         \"\"\"\n",
    "#         loc: loc of the normal distribution\n",
    "#         lam: precision of the normal distribution\n",
    "#         alpha: shape parameter of the inverse gamma distribution\n",
    "#         beta: rate parameter of the inverse gamma distribution\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.loc = torch.tensor(loc, dtype=torch.float32)\n",
    "#         self.lam = torch.tensor(lam, dtype=torch.float32)\n",
    "#         self.alpha = torch.tensor(alpha, dtype=torch.float32)\n",
    "#         self.beta = torch.tensor(beta, dtype=torch.float32)\n",
    "#         self.Temperature = torch.tensor(Temperature, dtype=torch.float32)\n",
    "#         self.name = \"Normal_Inverse_Gamma\"\n",
    "    \n",
    "\n",
    "#     def log_likelihood(self, values: torch.Tensor, var = torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Compute the likelihood of the inverse gamma distribution for an x and a variance\n",
    "#         \"\"\"\n",
    "#         # manually compute the likelihood\n",
    "#         # var =  self.beta / (self.alpha + 1 + 0.5) + 1e-8 # to avoid division by zero\n",
    "\n",
    "#         log_likelihood = torch.xlogy(0.5, self.lam / (2 * math.pi * var)) + \\\n",
    "#                     torch.xlogy(self.alpha,self.beta) - \\\n",
    "#                     torch.lgamma(self.alpha) - \\\n",
    "#                     torch.xlogy(self.alpha + 1,var) - \\\n",
    "#                     (2*self.beta + self.lam * (values - self.loc)**2) / (2 * var)\n",
    "        \n",
    "#         return log_likelihood/ self.Temperature\n",
    "\n",
    "#         #new try:\n",
    "#         # for all pos integers, gamma function:\n",
    "#         # F(a) = (a-1)!\n",
    "\n",
    "#         #var = self.beta / (self.alpha + 1 + 0.5)  # according to formula on wikipedia\n",
    "#         #part1 = self.lam ** 0.5 / (2 * np.pi * var) ** 0.5\n",
    "#         #part2 = self.beta ** self.alpha / np.math.factorial(self.alpha-1)\n",
    "#         #part3 = (1/var) ** (self.alpha + 1)\n",
    "#         #part4 = (-2*self.beta - self.lam * (values - self.mu) ** 0.5) / (2*var)\n",
    "#         #likelihood = part1 * part2 * part3 * np.exp(part4)\n",
    "#         #log_likelihood = np.log(likelihood)\n",
    "#         #return log_likelihood / self.Temperature\n",
    "\n",
    "\n",
    "#     def sample(self, n) -> torch.Tensor:\n",
    "#         # sample variance from inverse gamma and sample x from normal given the variance\n",
    "#         var = torch.div(1, dist.Gamma(self.alpha, self.beta).sample((1,)))\n",
    "#         x = dist.Normal(self.loc, torch.sqrt(var/self.lam)).sample((n,))\n",
    "#         return x, var\n",
    "\n",
    "\n",
    "# Spike and slab prior --------------------------------------------------------\n",
    "\n",
    "class GaussianSpikeNSlab(Prior):\n",
    "    \"\"\"\n",
    "    theta is the parameter for the bernoulli distribution\n",
    "    z ~ bern(theta)\n",
    "    if z=0, then x=0 approximately (\"spike distribution\", modelled as a very narrow normal distribution)\n",
    "    if z=1, then x ~ p_slab (\"slab distribution\")\n",
    "    We use the normal distribution as the slab distribution\n",
    "    p_theta(x) = theta * p_spike(x) + (1-theta) * p_slab(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loc_slab: float = 0, scale_slab: float = 1, loc_spike: float = 0, scale_spike: float = 1e-16, theta: float = 0.8, Temperature: float = 1.0,device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        loc_slab: mean of the normal distribution\n",
    "        scale_slab: standard deviation of the normal distribution\n",
    "        loc_spike: mean of the spike distribution\n",
    "        scale_spike: standard deviation of the spike distribution, should be very small to simulate a spike\n",
    "        theta: parameter of the bernoulli distribution for the mixture of the spike and the slab\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.theta = torch.tensor(theta, dtype=torch.float32, device = self.device)\n",
    "        self.loc_slab = torch.tensor(loc_slab, dtype=torch.float32, device = self.device)\n",
    "        self.scale_slab = torch.tensor(scale_slab, dtype=torch.float32, device = self.device)\n",
    "        self.loc_spike = torch.tensor(loc_spike, dtype=torch.float32, device = self.device)\n",
    "        self.scale_spike = torch.tensor(scale_spike, dtype=torch.float32, device = self.device)\n",
    "        self.Temperature = torch.tensor(Temperature, dtype=torch.float32, device = self.device)\n",
    "        self.name = \"Gaussian_Spike_and_Slab\"\n",
    "\n",
    "        mix = dist.Categorical(probs=torch.tensor([1-self.theta, self.theta],device=self.device))\n",
    "        comp = dist.Normal(torch.tensor([self.loc_spike, self.loc_slab], device = self.device), torch.tensor([self.scale_spike, self.scale_slab], device = self.device))\n",
    "        self.spike_n_slab = dist.MixtureSameFamily(mix, comp) \n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        return self.spike_n_slab.log_prob(values).sum() / self.Temperature\n",
    "\n",
    "    def sample(self,n) -> torch.Tensor:\n",
    "        return self.spike_n_slab.sample((n,))\n",
    "\n",
    "\n",
    "# Customized Laplace and Uniform Mixture prior --------------------------------\n",
    "\n",
    "\n",
    "## Uniform at the middle, Laplace at the sides, 50% weight on uniform, 25% weight each side.\n",
    "\n",
    "#   if x < -1:              f(x) = 0.67957*exp(x)\n",
    "#   if -1 <= x <= 1:        f(x) = 1/4\n",
    "#   if x > 1:               f(x) = 0.67957*exp(-x)\n",
    "\n",
    "\n",
    "class MixedLaplaceUniform(Prior):\n",
    "    \"\"\"\n",
    "    A mixture of Laplace and Uniform distributions.\n",
    "    we use a Uniform fistribution in the middle within interval [-1, 1] and a Laplace distribution at the sides.\n",
    "    The distribution is continuous. \n",
    "    \"\"\"\n",
    "    def __init__(self, Temperature:float=1.0):\n",
    "        super().__init__()\n",
    "        self.a = torch.exp(torch.tensor(1))/4\n",
    "        self.Temperature = Temperature\n",
    "        self.name = \"Mixed_Laplace_and_Uniform\"\n",
    "        \n",
    "    def log_likelihood(self, values: torch.tensor) -> torch.tensor:\n",
    "        log_likelihoods = torch.where(values < -1, values + torch.log(self.a), torch.where(values <= 1, torch.tensor(np.log(1/4)), -values + torch.log(self.a)))\n",
    "        return log_likelihoods.sum() / self.Temperature\n",
    "\n",
    "    def sample(self, size=1) -> torch.tensor:\n",
    "        \"\"\"Generates samples from the mixed probability distribution.\"\"\"\n",
    "        u = torch.rand(size)\n",
    "        first_case = torch.log(u/self.a)\n",
    "        second_case = (u - 1/4)*4 - 1\n",
    "        third_case = torch.log(1/(1/np.exp(1) - ((u-0.75)/self.a)))\n",
    "        samples = torch.where(u < 1/4, first_case, \n",
    "                              torch.where(u <= 3/4, second_case, third_case))\n",
    "        return samples\n",
    "\n",
    "\n",
    "\n",
    "# Pre-train the prior on FashionMNIST -----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\"\"\" Base Networks \"\"\"\n",
    "\n",
    "\"\"\" Fully Connected Neural Network \"\"\"\n",
    "\n",
    "\"\"\" - 3 Layers (2 Hidden, 1 Output)\n",
    "    - 100 units each for hidden layers\n",
    "    - ReLu activations\n",
    "    \"\"\"\n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, in_features = 28*28, out_features = 10, hidden_units = 100, hidden_layers = 2):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "\n",
    "        # Create layer list\n",
    "        self.layers = []\n",
    "\n",
    "        # Input to first layer\n",
    "        self.layers.append(nn.Linear(in_features, hidden_units))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(hidden_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_units, hidden_units))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_units, out_features))\n",
    "\n",
    "        # Convert to sequential\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,28*28)\n",
    "        output = self.layers(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "\"\"\" Convolutional Neural Network \"\"\"\n",
    "\n",
    "\"\"\" - 3 Layers (2 Hidden, 1 Output)\n",
    "    - First two layers: Convolutional Layers with 64 channels, 3x3 convolutions, followed by 2x2 MaxPooling\n",
    "    - All layers use ReLu activations\n",
    "    \"\"\"\n",
    "\n",
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNN, self).__init__()\n",
    "        \n",
    "        # Create Network\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=4, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        output = self.layers(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesianNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Bayesian Neural Network \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "from torchmetrics.functional import calibration_error\n",
    "from collections import deque, OrderedDict\n",
    "from tqdm import trange\n",
    "import copy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from SGLD import SGLD\n",
    "\n",
    "\n",
    "class BNN_MCMC:\n",
    "    def __init__(self, dataset_train, network, prior, Temperature = 1.,\n",
    "     num_epochs = 300, max_size = 100, burn_in = 100, lr = 1e-3, sample_interval = 1, device = \"cpu\"):\n",
    "        super(BNN_MCMC, self).__init__()\n",
    "\n",
    "        # set device \n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        # Hyperparameters and general parameters\n",
    "        self.Temperature = Temperature\n",
    "        self.learning_rate = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.burn_in = burn_in\n",
    "        self.sample_interval = sample_interval\n",
    "        self.max_size = max_size\n",
    "\n",
    "\n",
    "        self.batch_size = 128\n",
    "        self.print_interval = 50\n",
    "        \n",
    "        # Data Loader\n",
    "        self.data_loader = DataLoader(dataset_train, batch_size=self.batch_size, shuffle=True)\n",
    "        self.sample_size = dataset_train.__len__()\n",
    "\n",
    "        # Set Prior\n",
    "        self.prior = prior\n",
    "\n",
    "        # Initialize the network\n",
    "        self.network = network.to(self.device)\n",
    "\n",
    "        # Set optimizer\n",
    "        self.optimizer = SGLD(self.network.parameters(), lr=self.learning_rate, num_data=self.batch_size, temperature=self.Temperature)\n",
    "\n",
    "        # Scheduler for polynomially decreasing learning rates\n",
    "        self.scheduler = PolynomialLR(self.optimizer, total_iters = self.num_epochs, power = 0.5)\n",
    "\n",
    "        # Deque to store model samples\n",
    "        self.model_sequence = deque()\n",
    "\n",
    "    def train(self):\n",
    "        num_iter = 0\n",
    "        print('Training Model')\n",
    "\n",
    "        self.network.train()\n",
    "        progress_bar = trange(self.num_epochs)\n",
    "\n",
    "        N = torch.tensor(self.sample_size, device = self.device)\n",
    "        if self.prior.name == 'Normal Inverse Gamma':\n",
    "            n_params = 0\n",
    "            SS_params = 0\n",
    "\n",
    "        for _ in progress_bar:\n",
    "            num_iter += 1\n",
    "\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(self.data_loader):\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                self.network.zero_grad()\n",
    "                n = len(batch_x)\n",
    "\n",
    "                # Perform forward pass\n",
    "                current_logits = self.network(batch_x)\n",
    "\n",
    "                # Compute the NLL\n",
    "                nll = N/n*F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y)\n",
    "\n",
    "                # Compute the log prior\n",
    "                log_prior = torch.tensor(0,device=self.device, dtype=torch.float32)\n",
    "\n",
    "                # prior for Normal Inverse Gamma\n",
    "                if self.prior.name == 'Normal_Inverse_Gamma':\n",
    "                    param_list = torch.tensor([],device=self.device)\n",
    "                    for name, param in self.network.named_parameters():\n",
    "                        if param.requires_grad:\n",
    "                            param_list = torch.cat((param_list, param.view(-1)))\n",
    "                            \n",
    "                    current_var = torch.var(param_list)\n",
    "                    log_prior += self.prior.log_likelihood(param, current_var).sum()\n",
    "\n",
    "                else:\n",
    "                    for name, param in self.network.named_parameters():\n",
    "                        if param.requires_grad:\n",
    "                            # param=torch.tensor(param,device=self.device)\n",
    "                            log_prior += self.prior.log_likelihood(param).sum()\n",
    "                \n",
    "\n",
    "                # Calculate the loss\n",
    "                #loss = N/n*F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y) - log_prior\n",
    "                loss = nll - log_prior\n",
    "\n",
    "                # Backpropagate to get the gradients\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                # Update the weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update Metrics according to print_interval\n",
    "                if batch_idx % self.print_interval == 0:\n",
    "                    current_logits = self.network(batch_x)\n",
    "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
    "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item(),\n",
    "                    nll_loss=N/n*F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y).item(),\n",
    "                    log_prior_normalized = - log_prior.item(),\n",
    "                    lr = self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # Decrease lr based on scheduler\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Save the model samples if past the burn-in epochs according to sampling interval\n",
    "            if num_iter > self.burn_in and num_iter % self.sample_interval == 0:\n",
    "                self.model_sequence.append(copy.deepcopy(self.network))\n",
    "                # self.network.state_dict()\n",
    "\n",
    "            # If model_sequence to big, delete oldest model\n",
    "            if len(self.model_sequence) > self.max_size:\n",
    "                self.model_sequence.popleft()\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.network.eval()\n",
    "\n",
    "        # Sum predictions from all models in model_sequence\n",
    "        estimated_probability = torch.zeros((len(x), 10), device = self.device)\n",
    "\n",
    "        for model in self.model_sequence:\n",
    "\n",
    "            self.network.load_state_dict(model.state_dict())\n",
    "            logits = self.network(x).detach()\n",
    "            estimated_probability += F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Normalize the combined predictions to get average predictions\n",
    "        estimated_probability /= len(self.model_sequence)\n",
    "\n",
    "        assert estimated_probability.shape == (x.shape[0], 10)  \n",
    "        return estimated_probability\n",
    "    \n",
    "    def test_accuracy(self,x):\n",
    "        # test set\n",
    "        x_test = x[:][0].clone().detach().to(self.device)\n",
    "        y_test = x[:][1].clone().detach().to(self.device)      \n",
    "\n",
    "        # predicted probabilities\n",
    "        class_probs = self.predict_probabilities(x_test)\n",
    "\n",
    "        # accuracy\n",
    "        accuracy = (class_probs.argmax(axis=1) == y_test).float().mean()\n",
    "        return  accuracy.cpu().numpy()\n",
    "\n",
    "    def test_calibration(self,x):\n",
    "        # test set\n",
    "        x_test = x[:][0].clone().detach().to(self.device)\n",
    "        y_test = x[:][1].clone().detach().to(self.device)       \n",
    "\n",
    "        # predicted probabilities\n",
    "        class_probs = self.predict_probabilities(x_test)\n",
    "\n",
    "        calib_err = calibration_error(class_probs, y_test, n_bins = 30, task = \"multiclass\", norm=\"l1\", num_classes=10)\n",
    "        return calib_err.cpu().numpy()\n",
    "\n",
    "    def test_auroc(self,x):\n",
    "        # test set\n",
    "        x_test = x[:][0].clone().detach()\n",
    "        y_test = x[:][1].clone().detach()         \n",
    "\n",
    "        # predicted probabilities\n",
    "        class_probs = self.predict_probabilities(x_test)\n",
    "\n",
    "        auroc = roc_auc_score(y_test, class_probs, multi_class='ovr')\n",
    "        return auroc.cpu().numpy()\n",
    "\n",
    "    def get_metrics(self, x):\n",
    "        accuracy = self.test_accuracy(x)\n",
    "        calib_err = self.test_calibration(x)\n",
    "        auroc = self.test_auroc(x)\n",
    "\n",
    "        return accuracy, calib_err, auroc\n",
    "\n",
    "    def get_posterior_stats(self):\n",
    "        self.network.eval()\n",
    "\n",
    "        # get weights from all models\n",
    "        param_flat_all = torch.tensor([],device = self.device)\n",
    "        for model in self.model_sequence:\n",
    "            parameters = model.state_dict()\n",
    "            param_values = list(parameters.values())\n",
    "            param_flat = torch.cat([v.flatten() for v in param_values])\n",
    "            param_flat_all.append(param_flat.flatten())\n",
    "\n",
    "        param_flat_all = torch.cat(param_flat_all)\n",
    "\n",
    "        # get mean and variance\n",
    "        mean = torch.mean(param_flat_all, dim=0)\n",
    "        var = torch.var(param_flat_all, dim=0)\n",
    "\n",
    "\n",
    "        return mean.cpu().numpy(), var.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script runs the experiment for our project.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Importing libraries ---------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as tr\n",
    "import pandas as pd\n",
    "\n",
    "from data import Data\n",
    "from priors import *\n",
    "from Networks import *\n",
    "from BayesianNN import BNN_MCMC\n",
    "\n",
    "# Setting seeds ---------------------------------------------------------------\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda\"\n",
    "\n",
    "# Specify the prior -----------------------------------------------------------\n",
    "\n",
    "# Possible prior choices: \n",
    "#       Isotropic_Gaussian, \n",
    "#       StudentT_prior\n",
    "#       Laplace_prior\n",
    "#       Gaussian_Mixture\n",
    "#       Normal_Inverse_Gamma\n",
    "#       GaussianSpikeNSlab\n",
    "#       MixedLaplaceUniform\n",
    "\n",
    "prior = GaussianSpikeNSlab()\n",
    "\n",
    "\n",
    "# Specify the iteration parameters --------------------------------------------\n",
    "\n",
    "# network list\n",
    "networks = {\"FCNN\": FullyConnectedNN(), \"CNN\": ConvolutionalNN()}\n",
    "\n",
    "# Temperature list\n",
    "Temperatures = [0.001, 0.01, 0.1, 1.]\n",
    "\n",
    "\n",
    "# sample size list\n",
    "sample_sizes = [3750, 15000, 60000, 120000]\n",
    "\n",
    "# preallocate pandas dataframe for results\n",
    "results = pd.DataFrame(columns = [\n",
    "    \"Network\", \n",
    "    \"Sample Size\", \n",
    "    \"Epochs\", \n",
    "    \"Burn in\", \n",
    "    \"sample interval\", \n",
    "    \"Temperature\", \n",
    "    \"Test Accuracy\", \n",
    "    \"Test ECE\", \n",
    "    \"Test AUROC\",\n",
    "    \"Posterior mean\",\n",
    "    \"Posterior var\"],\n",
    "    index = range(len(networks)*len(Temperatures)*len(sample_sizes)))\n",
    "\n",
    "\n",
    "#create a dict for the different parameter values\n",
    "base_epoch, base_burn_in, base_sample_interval, base_samplesize = 50, 10, 2, sample_sizes[-1]\n",
    "args_dict = [(sample_size, (base_epoch*base_samplesize/sample_size, base_burn_in*base_samplesize/sample_size, base_sample_interval*base_samplesize/sample_size )) for sample_size in sample_sizes]\n",
    "args_dict = dict(args_dict)\n",
    "\n",
    "# Run the experiment ----------------------------------------------------------\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "for net in networks.keys():\n",
    "    for T in Temperatures:\n",
    "        for n in range(len(sample_sizes)):\n",
    "        \n",
    "\n",
    "            # print iteration info\n",
    "            print(50*\"-\")\n",
    "            print(\"Iteration: \", iteration + 1, \" of \", len(networks)*len(Temperatures)*len(sample_sizes))\n",
    "            print(\"Network:     \", net)\n",
    "            print(\"Prior:       \", prior.name)\n",
    "            print(\"Temperature: \", T)\n",
    "            \"\"\"\n",
    "            print(\"Sample size: \", sample_sizes[n])\n",
    "            print(\"Epoch:       \", args_dict[sample_sizes[n]][0])\n",
    "            print(\"Burn in:     \", args_dict[sample_sizes[n]][1])\n",
    "            print(\"Sample interval: \", args_dict[sample_sizes[n]][2])\n",
    "            \"\"\"\n",
    "\n",
    "            # get data\n",
    "            if sample_sizes[n] == 120000:\n",
    "                # if sample size is 120000, use data augmentation\n",
    "                augmentations = tr.Compose([tr.RandomRotation(15)])\n",
    "                train_data, test_data = Data(\"MNIST\", augmentations = augmentations).get_data(num_train_samples=sample_sizes[n])\n",
    "            else:\n",
    "                # subsample original train data if sample size is smaller than 120000\n",
    "                train_data, test_data = Data(\"MNIST\", augmentations = None).get_data(num_train_samples=sample_sizes[n])\n",
    "\n",
    "\n",
    "            # run BNN\n",
    "            model = BNN_MCMC(\n",
    "                train_data,\n",
    "                network = networks[net],\n",
    "                prior=prior,\n",
    "                Temperature = T,\n",
    "                num_epochs = int(args_dict[sample_sizes[n]][0]),\n",
    "                max_size = 20,\n",
    "                burn_in = int(args_dict[sample_sizes[n]][1]),\n",
    "                lr = 1e-3,\n",
    "                sample_interval = int(args_dict[sample_sizes[n]][2]),\n",
    "                device = device)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            # get test metrics\n",
    "            acc, ece, auroc = model.get_metrics(test_data)\n",
    "            post_mean, post_var = model.get_posterior_stats()\n",
    "\n",
    "            #print(\"Test accuracy: \", acc)\n",
    "            #print(\"Test ECE: \", ece)\n",
    "            #print(\"Test AUROC: \", auroc)\n",
    "\n",
    "            # save results\n",
    "            results.iloc[iteration, :] = net, sample_sizes[n], args_dict[sample_sizes[n]][0], args_dict[sample_sizes[n]][1], args_dict[sample_sizes[n]][2], T, acc, ece, auroc, post_mean, post_var\n",
    "            iteration += 1\n",
    "\n",
    "# save results to csv\n",
    "results.to_csv(f\"results/results_{prior.name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
