{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.distributions as dist\n",
    "import abc\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions.distribution import Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature = trainset[0][0]\n",
    "label = trainset[0][1]\n",
    "print(label)\n",
    "plt.imshow(feature.squeeze(), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will first implement a BNN such as in the PAI Task 2. Thus it will be a BNN that is trained via Bayes by Backprop. We will need to change this to train the BNN via SG-MCMC as we said in the Proposal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define the Prior distributions which we will later use in the BNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior(nn.Module, abc.ABC):\n",
    "    \"\"\"\n",
    "    https://github.com/ratschlab/bnn_priors/blob/main/bnn_priors/prior/base.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_likelihood(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the given x values\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from the prior\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Don't use this method, we only implement it because nn.Module requires it\n",
    "        Vincent Fortuin uses the forward to return the parameter value using self.p\n",
    "        \"\"\"\n",
    "        return self.log_likelihood(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Isotropic_Gaussian2(Distribution):\n",
    "    \"\"\" CAVEAT: Not sure if this is correct. Probably not even needed. Just use dist.normal\"\"\"\n",
    "    def __init__(self, loc: torch.Tensor, scale: torch.Tensor):\n",
    "        super(Isotropic_Gaussian, self).__init__()\n",
    "        assert scale > 0, \"Scale must be positive.\"\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "\n",
    "    def log_prob(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        return -0.5 * torch.log(2 * torch.pi * self.scale ** 2) - (values - self.loc) ** 2 / (2 * self.scale ** 2)\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        return torch.normal(self.loc, self.scale, sample_shape)\n",
    "\n",
    "    def entropy(self):\n",
    "        return 0.5 * torch.log(2 * torch.pi * torch.e) + torch.log(self.scale)\n",
    "\n",
    "    def kl_divergence(self, other):\n",
    "        return torch.log(self.scale / other.scale) + (other.scale ** 2 + (self.loc - other.loc) ** 2) / (2 * self.scale ** 2) - 0.5\n",
    "\n",
    "    def mean(self):\n",
    "        return self.loc\n",
    "\n",
    "    def std(self):\n",
    "        return self.scale\n",
    "\n",
    "    def mode(self):\n",
    "        return self.loc\n",
    "\n",
    "\n",
    "\n",
    "class Isotropic_Gaussian(Distribution):\n",
    "    \"\"\"\n",
    "    https://github.com/ratschlab/bnn_priors/blob/main/bnn_priors/prior/loc_scale.py\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a basic module for one single Bayesian Layer. This module will then be used as building block for the full BNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 952.23, Accuracy: 0.12: 100%|██████████| 15/15 [07:02<00:00, 28.20s/it] \n"
     ]
    }
   ],
   "source": [
    "class Linear_Layer(nn.Linear):\n",
    "    \"\"\"\n",
    "    Bayesian Linear Layer that will be used as a building block for the Bayesian Neural Network\n",
    "    https://github.com/ratschlab/bnn_priors/blob/main/bnn_priors/models/layers.py\n",
    "    \"\"\" \n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.with_bias = bias\n",
    "\n",
    "        # create a prior for the weights and biases using the Isotropic Gaussian prior\n",
    "        self.weight_prior = Isotropic_Gaussian(\n",
    "            mu = torch.zeros(out_features, in_features),\n",
    "            sigma = torch.ones(out_features, in_features)\n",
    "        )\n",
    "\n",
    "        if bias:\n",
    "            self.bias_prior = Isotropic_Gaussian(\n",
    "                mu = torch.zeros(out_features),\n",
    "                sigma = torch.ones(out_features)\n",
    "            )\n",
    "        \n",
    "        # create a variational posterior for the weights and biases as Instance of Isotropic Gaussian\n",
    "        self.weight_posterior = Isotropic_Gaussian(\n",
    "            mu = torch.nn.Parameter(torch.zeros(out_features, in_features)),\n",
    "            sigma = torch.nn.Parameter(torch.ones(out_features, in_features))\n",
    "        )\n",
    "\n",
    "        if bias:\n",
    "            self.bias_posterior = Isotropic_Gaussian(\n",
    "                mu = torch.nn.Parameter(torch.zeros(out_features)),\n",
    "                sigma = torch.nn.Parameter(torch.ones(out_features))\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # sample weights and biases from the variational posterior\n",
    "        weight = self.weight_posterior.sample()\n",
    "        bias = self.bias_posterior.sample() if self.with_bias else None\n",
    "\n",
    "        # compute the log-likelihood of the weights and biases\n",
    "        log_prior = self.weight_prior.log_likelihood(weight) + self.bias_prior.log_likelihood(bias)\n",
    "\n",
    "        # compute the log posterior of the weights and biases\n",
    "        log_posterior = self.weight_posterior.log_likelihood(weight) + self.bias_posterior.log_likelihood(bias)\n",
    "\n",
    "        # compute the output of the layer\n",
    "        output = F.linear(x, weight, bias)\n",
    "\n",
    "        return output, log_prior, log_posterior    \n",
    "\n",
    "\n",
    "class Bayesian_Neural_Network(nn.Module):\n",
    "    \"\"\"\n",
    "    Bayesian Neural Network that will be trained using the BNN implementation\n",
    "    \"\"\" \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # create the layers of the network\n",
    "        self.input_layer = Linear_Layer(in_features=self.input_size, out_features=self.hidden_size)\n",
    "        self.hidden_layer = Linear_Layer(in_features=self.hidden_size, out_features=self.hidden_size)\n",
    "        self.output_layer = Linear_Layer(in_features=self.hidden_size, out_features=self.output_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # compute the output of the input layer\n",
    "        x, log_prior, log_posterior = self.input_layer(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # compute the output of the hidden layer\n",
    "        x, log_prior, log_posterior = self.hidden_layer(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # compute the output of the output layer\n",
    "        x, log_prior, log_posterior = self.output_layer(x)\n",
    "\n",
    "        return x, log_prior, log_posterior\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # compute the output of the network\n",
    "        x, _, _ = self.forward(x)\n",
    "\n",
    "        # compute the class probabilities\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict_class(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # compute the class probabilities\n",
    "        x = self.predict(x)\n",
    "\n",
    "        # compute the class predictions\n",
    "        x = x.argmax(dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# create the Bayesian Neural Network\n",
    "model = Bayesian_Neural_Network(input_size=28*28, hidden_size=100, output_size=10)\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# create the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# train the model\n",
    "epochs = 15\n",
    "progress_bar = tqdm.trange(epochs)\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    for x, y in train_loader:\n",
    "        # reshape the input\n",
    "        x = x.view(-1, 28*28)\n",
    "\n",
    "        # compute the output of the network\n",
    "        y_hat, log_prior, log_posterior = model(x)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_fn(y_hat, y) - log_prior + log_posterior\n",
    "\n",
    "        # compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # compute the accuracy\n",
    "    accuracy = (model.predict_class(x) == y).float().mean()\n",
    "\n",
    "    # update the progress bar\n",
    "    progress_bar.set_description(f\"Epoch: {epoch+1}, Loss: {loss.item():.2f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "# some changes\n",
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Layer(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    Bayesian Convolutional Layer that will be used as a building block for the Bayesian Neural Network\n",
    "    https://github.com/ratschlab/bnn_priors/blob/main/bnn_priors/models/layers.py\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = True):\n",
    "        super(Conv_Layer, self).__init__()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bayesian_FCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Bayesian Fully Connected Neural Network\n",
    "    https://arxiv.org/pdf/2102.06571.pdf\n",
    "    https://github.com/ratschlab/bnn_priors/blob/main/bnn_priors/models/dense_nets.py\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_sizes: list, activation: str = 'relu'):\n",
    "        super(Bayesian_FCN, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def sample_elbo(self, input: torch.Tensor, target: torch.Tensor, samples: int = 1, beta: float = 1.0) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def sample_predict(self, input: torch.Tensor, samples: int = 1) -> torch.Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bayesian_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Bayesian Convolutional Neural Network\n",
    "    https://github.com/ratschlab/bnn_priors/blob/main/bnn_priors/models/conv_nets.py\n",
    "    https://arxiv.org/pdf/2102.06571.pdf\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_sizes: list, activation: str = 'relu'):\n",
    "        super(Bayesian_CNN, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def sample_elbo(self, input: torch.Tensor, target: torch.Tensor, samples: int = 1, beta: float = 1.0) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def sample_predict(self, input: torch.Tensor, samples: int = 1) -> torch.Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "9412b45e6a57aa9914730508726d49801d3b2c579f461e1fb13c705887a7b1f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
