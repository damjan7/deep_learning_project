{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.distributions as dist\n",
    "import abc\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions.distribution import Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "x_train = torch.from_numpy(trainset.data.numpy().reshape(-1, 28*28))/255.\n",
    "y_train = torch.from_numpy(trainset.targets.numpy())\n",
    "train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "x_test = torch.from_numpy(testset.data.numpy().reshape(-1, 28*28))/255.\n",
    "y_test = torch.from_numpy(testset.targets.numpy())\n",
    "test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature = trainset[0][0]\n",
    "label = trainset[0][1]\n",
    "print(label)\n",
    "plt.imshow(feature.squeeze(), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will first implement a BNN such as in the PAI Task 2. Thus it will be a BNN that is trained via Bayes by Backprop. We will need to change this to train the BNN via SG-MCMC as we said in the Proposal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define the Prior distributions which we will later use in the BNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior(nn.Module, abc.ABC):\n",
    "    \"\"\"\n",
    "    This class is a base class for all priors.\n",
    "    It implements the log_likelihood and sample methods.\n",
    "    The forward method is not used, but is required by nn.Module.\n",
    "    This part of the code is inspired by the code from Vincent Fortuin:\n",
    "    https://github.com/ratschlab/bnn_priors/blob/main/bnn_priors/prior/base.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_likelihood(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the given x values\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from the prior\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Don't use this method, we only implement it because nn.Module requires it\n",
    "        Vincent Fortuin uses the forward to return the parameter value using self.p\n",
    "        \"\"\"\n",
    "        return self.log_likelihood(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Isotropic_Gaussian(Prior):\n",
    "    \"\"\"\n",
    "    Isotropic Gaussian prior\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: torch.Tensor, sigma: torch.Tensor):\n",
    "        super(Isotropic_Gaussian, self).__init__()\n",
    "        #assert all(sigma >= torch.zeros(sigma.size())), \"Sigma must be positive\"\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def log_likelihood(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the given x values\n",
    "        \"\"\"\n",
    "        return dist.Normal(self.mu, self.sigma).log_prob(x).sum()\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from the prior\n",
    "        \"\"\"\n",
    "        return dist.Normal(self.mu, self.sigma).sample()\n",
    "\n",
    "\n",
    "\n",
    "class Multivariate_Gaussian(Prior):\n",
    "    \"\"\"\n",
    "    Multivariate Gaussian prior\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: torch.Tensor, sigma: torch.Tensor):\n",
    "        super(Multivariate_Gaussian, self).__init__()\n",
    "        #assert sigma.shape == mu.shape, \"Sigma and mu must have the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def log_likelihood(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the given x values\n",
    "        \"\"\"\n",
    "        sigma_resized = torch.reshape(self.sigma, (-1, ))\n",
    "        mu_resized = torch.reshape(self.mu, (-1, ))\n",
    "        print(sigma_resized.shape)\n",
    "        covariance = torch.diag(sigma_resized)\n",
    "\n",
    "        return dist.MultivariateNormal(mu_resized, covariance).log_prob(x).sum()\n",
    "\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from the prior\n",
    "        \"\"\"\n",
    "        sigma_resized = torch.reshape(self.sigma, (-1, ))\n",
    "        mu_resized = torch.reshape(self.mu, (-1, ))\n",
    "        covariance = torch.diag(sigma_resized)  \n",
    "\n",
    "        return dist.MultivariateNormal(mu_resized, covariance).sample()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a basic module for one single Bayesian Layer. This module will then be used as building block for the full BNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Layer(nn.Linear):\n",
    "    \"\"\"\n",
    "    Bayesian Linear Layer that will be used as a building block for the Bayesian Neural Network\n",
    "    https://github.com/ratschlab/bnn_priors/blob/main/bnn_priors/models/layers.py\n",
    "    \"\"\" \n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.with_bias = bias\n",
    "\n",
    "        # create a prior for the weights and biases using the Isotropic Gaussian prior\n",
    "        self.weight_prior = Isotropic_Gaussian(\n",
    "            mu = torch.zeros(out_features, in_features),\n",
    "            sigma = torch.ones(out_features, in_features)\n",
    "        )\n",
    "\n",
    "        if self.with_bias:\n",
    "            self.bias_prior = Isotropic_Gaussian(\n",
    "                mu = torch.zeros(out_features),\n",
    "                sigma = torch.ones(out_features)\n",
    "            )\n",
    "        \n",
    "        # create a variational posterior for the weights and biases as Instance of  Multivariate Gaussian\n",
    "        self.weight_posterior = Isotropic_Gaussian(\n",
    "            mu = torch.nn.Parameter(torch.zeros(out_features, in_features)),\n",
    "            sigma = torch.nn.Parameter(torch.ones(out_features, in_features))\n",
    "        )\n",
    "        \n",
    "        if self.with_bias:\n",
    "            self.bias_posterior = Isotropic_Gaussian(\n",
    "                mu = torch.nn.Parameter(torch.zeros(out_features)),\n",
    "                sigma = torch.nn.Parameter(torch.ones(out_features))\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Same procedure as explained in the paper of Blundell et al. (2015):\n",
    "        use reparameterization trick to sample weights and biases from the variational posterior\n",
    "        \"\"\"\n",
    "        # usample random noise from the standard normal distribution\n",
    "        epsilon = torch.randn_like(self.weight_posterior.mu)\n",
    "\n",
    "        # sample weights and biases from the variational posterior\n",
    "        weight = self.weight_posterior.mu + torch.multiply(epsilon, self.weight_posterior.sigma)\n",
    "\n",
    "        # compute the log prior of the weights and biases\n",
    "        log_prior = self.weight_prior.log_likelihood(weight)\n",
    "\n",
    "        # compute the log variational posterior of the weights and biases\n",
    "        log_posterior = self.weight_posterior.log_likelihood(weight)\n",
    "\n",
    "        # adjust for the bias\n",
    "        if self.with_bias:\n",
    "            epsilon = torch.randn_like(self.bias_posterior.mu)\n",
    "            bias = self.bias_posterior.mu + torch.multiply(epsilon, self.bias_posterior.sigma)\n",
    "            log_prior += self.bias_prior.log_likelihood(bias)\n",
    "            log_posterior += self.bias_posterior.log_likelihood(bias)\n",
    "        else:\n",
    "            bias = None\n",
    "\n",
    "\n",
    "        kl_divergence = log_posterior - log_prior\n",
    "        \n",
    "        # compute the output of the layer\n",
    "        output = F.linear(x, weight, bias)\n",
    "        return output, kl_divergence\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Bayesian_Neural_Network(nn.Module):\n",
    "    \"\"\"\n",
    "    Bayesian Neural Network that will be trained using the BNN implementation\n",
    "    \"\"\" \n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        # create the layers of the network\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # create the input layer\n",
    "        self.layers.append(Linear_Layer(self.input_dim, self.hidden_dims[0]))\n",
    "        #self.layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        # create the hidden layers\n",
    "        for i in range(len(self.hidden_dims) - 1):\n",
    "            self.layers.append(Linear_Layer(self.hidden_dims[i], self.hidden_dims[i + 1]))\n",
    "            #self.layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        # create the output layer\n",
    "        self.layers.append(Linear_Layer(self.hidden_dims[-1], self.output_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the network\n",
    "        \"\"\"\n",
    "        kl_divergence = 0\n",
    "        for ind, layer in enumerate(self.layers):\n",
    "            x, kl = layer(x)\n",
    "            kl_divergence += kl\n",
    "            if ind < len(self.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "\n",
    "        return x, kl_divergence\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict the output of the network and return the output\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            output, _ = self.forward(x)\n",
    "            return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian_Neural_Network(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear_Layer(\n",
      "      in_features=784, out_features=100, bias=True\n",
      "      (weight_prior): Isotropic_Gaussian()\n",
      "      (bias_prior): Isotropic_Gaussian()\n",
      "      (weight_posterior): Isotropic_Gaussian()\n",
      "      (bias_posterior): Isotropic_Gaussian()\n",
      "    )\n",
      "    (1): Linear_Layer(\n",
      "      in_features=100, out_features=10, bias=True\n",
      "      (weight_prior): Isotropic_Gaussian()\n",
      "      (bias_prior): Isotropic_Gaussian()\n",
      "      (weight_posterior): Isotropic_Gaussian()\n",
      "      (bias_posterior): Isotropic_Gaussian()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Epoch: 0 | Batch: 0 | Loss: -0.09515979886054993\n",
      "Epoch: 0 | Batch: 100 | Loss: 16.86176300048828\n",
      "Epoch: 0 | Batch: 200 | Loss: 11.615659713745117\n",
      "Epoch: 0 | Batch: 300 | Loss: 20.14438819885254\n",
      "Epoch: 0 | Batch: 400 | Loss: 35.73916244506836\n",
      "Epoch: 0 | Batch: 500 | Loss: 27.11829376220703\n",
      "Epoch: 0 | Batch: 600 | Loss: 49.077762603759766\n",
      "Epoch: 0 | Batch: 700 | Loss: 43.770992279052734\n",
      "Epoch: 0 | Batch: 800 | Loss: 43.54045104980469\n",
      "Epoch: 0 | Batch: 900 | Loss: 50.0528564453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:18<02:43, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch: 0 | Loss: 48.598411560058594\n",
      "Epoch: 1 | Batch: 100 | Loss: 32.84897232055664\n",
      "Epoch: 1 | Batch: 200 | Loss: 47.80905532836914\n",
      "Epoch: 1 | Batch: 300 | Loss: 52.07752227783203\n",
      "Epoch: 1 | Batch: 400 | Loss: 58.828125\n",
      "Epoch: 1 | Batch: 500 | Loss: 37.8927001953125\n",
      "Epoch: 1 | Batch: 600 | Loss: 47.0234375\n",
      "Epoch: 1 | Batch: 700 | Loss: 60.4962158203125\n",
      "Epoch: 1 | Batch: 800 | Loss: 37.74379348754883\n",
      "Epoch: 1 | Batch: 900 | Loss: 45.35428237915039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:34<02:21, 17.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Batch: 0 | Loss: 48.692848205566406\n",
      "Epoch: 2 | Batch: 100 | Loss: 43.869686126708984\n",
      "Epoch: 2 | Batch: 200 | Loss: 41.08445358276367\n",
      "Epoch: 2 | Batch: 300 | Loss: 60.556640625\n",
      "Epoch: 2 | Batch: 400 | Loss: 50.426170349121094\n",
      "Epoch: 2 | Batch: 500 | Loss: 42.48792266845703\n",
      "Epoch: 2 | Batch: 600 | Loss: 36.6080322265625\n",
      "Epoch: 2 | Batch: 700 | Loss: 33.66780471801758\n",
      "Epoch: 2 | Batch: 800 | Loss: 51.15086364746094\n",
      "Epoch: 2 | Batch: 900 | Loss: 48.3382568359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:55<02:10, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Batch: 0 | Loss: 48.87843322753906\n",
      "Epoch: 3 | Batch: 100 | Loss: 59.371829986572266\n",
      "Epoch: 3 | Batch: 200 | Loss: 46.99433898925781\n",
      "Epoch: 3 | Batch: 300 | Loss: 28.010833740234375\n",
      "Epoch: 3 | Batch: 400 | Loss: 42.119693756103516\n",
      "Epoch: 3 | Batch: 500 | Loss: 61.715606689453125\n",
      "Epoch: 3 | Batch: 600 | Loss: 39.11267852783203\n",
      "Epoch: 3 | Batch: 700 | Loss: 59.35221481323242\n",
      "Epoch: 3 | Batch: 800 | Loss: 47.64887237548828\n",
      "Epoch: 3 | Batch: 900 | Loss: 36.8270263671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:15<01:54, 19.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Batch: 0 | Loss: 51.71307373046875\n",
      "Epoch: 4 | Batch: 100 | Loss: 38.1565055847168\n",
      "Epoch: 4 | Batch: 200 | Loss: 41.11244201660156\n",
      "Epoch: 4 | Batch: 300 | Loss: 53.61478805541992\n",
      "Epoch: 4 | Batch: 400 | Loss: 39.54041290283203\n",
      "Epoch: 4 | Batch: 500 | Loss: 46.080528259277344\n",
      "Epoch: 4 | Batch: 600 | Loss: 57.41802215576172\n",
      "Epoch: 4 | Batch: 700 | Loss: 43.958316802978516\n",
      "Epoch: 4 | Batch: 800 | Loss: 38.133548736572266\n",
      "Epoch: 4 | Batch: 900 | Loss: 53.07054901123047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:45<01:51, 22.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Batch: 0 | Loss: 54.239776611328125\n",
      "Epoch: 5 | Batch: 100 | Loss: 56.714599609375\n",
      "Epoch: 5 | Batch: 200 | Loss: 44.83207702636719\n",
      "Epoch: 5 | Batch: 300 | Loss: 61.292823791503906\n",
      "Epoch: 5 | Batch: 400 | Loss: 36.64524459838867\n",
      "Epoch: 5 | Batch: 500 | Loss: 28.873455047607422\n",
      "Epoch: 5 | Batch: 600 | Loss: 58.31439208984375\n",
      "Epoch: 5 | Batch: 700 | Loss: 45.44308853149414\n",
      "Epoch: 5 | Batch: 800 | Loss: 47.891597747802734\n",
      "Epoch: 5 | Batch: 900 | Loss: 45.1481819152832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:08<01:29, 22.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Batch: 0 | Loss: 38.04106140136719\n",
      "Epoch: 6 | Batch: 100 | Loss: 56.051666259765625\n",
      "Epoch: 6 | Batch: 200 | Loss: 34.644676208496094\n",
      "Epoch: 6 | Batch: 300 | Loss: 51.28498840332031\n",
      "Epoch: 6 | Batch: 400 | Loss: 37.65415573120117\n",
      "Epoch: 6 | Batch: 500 | Loss: 52.9639892578125\n",
      "Epoch: 6 | Batch: 600 | Loss: 35.021385192871094\n",
      "Epoch: 6 | Batch: 700 | Loss: 40.820106506347656\n",
      "Epoch: 6 | Batch: 800 | Loss: 47.53953170776367\n",
      "Epoch: 6 | Batch: 900 | Loss: 48.847660064697266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:29<01:06, 22.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Batch: 0 | Loss: 51.4085693359375\n",
      "Epoch: 7 | Batch: 100 | Loss: 50.15510559082031\n",
      "Epoch: 7 | Batch: 200 | Loss: 41.41136932373047\n",
      "Epoch: 7 | Batch: 300 | Loss: 48.4462890625\n",
      "Epoch: 7 | Batch: 400 | Loss: 67.11390686035156\n",
      "Epoch: 7 | Batch: 500 | Loss: 48.664180755615234\n",
      "Epoch: 7 | Batch: 600 | Loss: 40.53447723388672\n",
      "Epoch: 7 | Batch: 700 | Loss: 58.56099319458008\n",
      "Epoch: 7 | Batch: 800 | Loss: 51.439453125\n",
      "Epoch: 7 | Batch: 900 | Loss: 36.07307052612305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:48<00:42, 21.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Batch: 0 | Loss: 51.29830551147461\n",
      "Epoch: 8 | Batch: 100 | Loss: 36.28865432739258\n",
      "Epoch: 8 | Batch: 200 | Loss: 34.14769744873047\n",
      "Epoch: 8 | Batch: 300 | Loss: 56.631195068359375\n",
      "Epoch: 8 | Batch: 400 | Loss: 37.91389465332031\n",
      "Epoch: 8 | Batch: 500 | Loss: 50.876380920410156\n",
      "Epoch: 8 | Batch: 600 | Loss: 36.20116424560547\n",
      "Epoch: 8 | Batch: 700 | Loss: 50.550228118896484\n",
      "Epoch: 8 | Batch: 800 | Loss: 44.772300720214844\n",
      "Epoch: 8 | Batch: 900 | Loss: 55.46240997314453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:12<00:22, 22.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Batch: 0 | Loss: 59.09617614746094\n",
      "Epoch: 9 | Batch: 100 | Loss: 48.49892044067383\n",
      "Epoch: 9 | Batch: 200 | Loss: 43.729331970214844\n",
      "Epoch: 9 | Batch: 300 | Loss: 58.735713958740234\n",
      "Epoch: 9 | Batch: 400 | Loss: 40.561767578125\n",
      "Epoch: 9 | Batch: 500 | Loss: 45.02233123779297\n",
      "Epoch: 9 | Batch: 600 | Loss: 47.851173400878906\n",
      "Epoch: 9 | Batch: 700 | Loss: 72.2313003540039\n",
      "Epoch: 9 | Batch: 800 | Loss: 43.877197265625\n",
      "Epoch: 9 | Batch: 900 | Loss: 60.916175842285156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:34<00:23, 23.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d796f27ba788>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jorge\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jorge\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create the Bayesian Neural Network\n",
    "model = Bayesian_Neural_Network(input_dim = 28*28, output_dim = 10, hidden_dims = [100])\n",
    "print(model)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# train the model\n",
    "epochs = 10\n",
    "\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # reshape the data\n",
    "        x = x.view(-1, 28*28)\n",
    "\n",
    "        # forward pass\n",
    "        output, kl_divergence = model(x)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.nll_loss(F.softmax(output, dim = 1), y) + kl_divergence\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # print the loss\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch: {epoch} | Batch: {i} | Loss: {loss.item()}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6146560000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create the Bayesian Neural Network\n",
    "model = Bayesian_Neural_Network(input_size=28*28, hidden_size=100, output_size=10)\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# create the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# train the model\n",
    "epochs = 15\n",
    "progress_bar = tqdm(epochs)\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    for x, y in train_loader:\n",
    "        # reshape the input\n",
    "        x = x.view(-1, 28*28)\n",
    "\n",
    "        # compute the output of the network\n",
    "        y_hat, log_prior, log_posterior = model(x)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_fn(y_hat, y) - log_prior + log_posterior\n",
    "\n",
    "        # compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # compute the accuracy\n",
    "    accuracy = (model.predict_class(x) == y).float().mean()\n",
    "\n",
    "    # update the progress bar\n",
    "    progress_bar.set_description(f\"Epoch: {epoch+1}, Loss: {loss.item():.2f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Layer(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    Bayesian Convolutional Layer that will be used as a building block for the Bayesian Neural Network\n",
    "    https://github.com/ratschlab/bnn_priors/blob/main/bnn_priors/models/layers.py\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = True):\n",
    "        super(Conv_Layer, self).__init__()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "9412b45e6a57aa9914730508726d49801d3b2c579f461e1fb13c705887a7b1f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
