{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim import SGD\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "import tqdm\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "transform = transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample from trainset\n",
    "n_subsamples_train = 2000 # size of subset\n",
    "sub_train_idx = random.sample(range(60000),n_subsamples_train) # 60'000 is train size in MNSIT\n",
    "sub_train_set = Subset(train_set, sub_train_idx)\n",
    "\n",
    "# subsample from testset\n",
    "n_subsamples_test = 1000  # size of subset\n",
    "sub_test_idx = random.sample(range(10000), n_subsamples_test) # 10'000 is test size in MNIST\n",
    "sub_test_set = Subset(test_set, sub_test_idx)\n",
    "\n",
    "# load\n",
    "#sub_train_dataloader = DataLoader(sub_train_set, batch_size=64, shuffle=True)\n",
    "sub_test_dataloader = DataLoader(sub_test_set, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick Check \n",
    "feature_test = sub_test_set[999][0]\n",
    "label_test = sub_test_set[999][1]\n",
    "print(label_test)\n",
    "plt.imshow(feature_test.squeeze(), cmap='hot')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "feature_train = sub_train_set[1999][0]\n",
    "label_train = sub_train_set[1999][1]\n",
    "print(label_train)\n",
    "plt.imshow(feature_train.squeeze(), cmap='hot')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Model Part Krause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Class for Priors\n",
    "\n",
    "class Prior:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def sample(self,n):\n",
    "        pass\n",
    "    def log_likelihood(self,values):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Prior\n",
    "# Change: to a subclass of Prior\n",
    "\n",
    "class IsotropicGaussian(Prior):\n",
    "    def __init__(self, mean=0, std=1):\n",
    "        super(IsotropicGaussian,self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def sample(self, n):\n",
    "        return np.random.normal(self.mean, self.std, size=n)\n",
    "\n",
    "    def log_likelihood(self, weights):\n",
    "        return Normal(self.mean, self.std).log_prob(torch.tensor(weights)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Neural Network \n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, in_features = 28*28, out_features = 10, hidden_units = 100, hidden_layers = 3):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(in_features, hidden_units))\n",
    "        for i in range(hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_units, hidden_units))\n",
    "        self.output_layer = nn.Linear(hidden_units, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,28*28)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        class_probs = self.output_layer(x)\n",
    "        return class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework\n",
    "\n",
    "class Framework(object):\n",
    "    def __init__(self, training_set, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Basic Framework for your bayesian neural network.\n",
    "        SGLD will be based on this.\n",
    "        \"\"\"\n",
    "        self.train_set = training_set\n",
    "        self.print_interval = 64 # number of batches until updated metrics are displayed during training\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict(self, data_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the class probabilities using your trained model.\n",
    "        This method should return an (num_samples, 10) NumPy float array\n",
    "        such that the second dimension sums up to 1 for each row.\n",
    "\n",
    "        :param data_loader: Data loader yielding the samples to predict on\n",
    "        :return: (num_samples, 10) NumPy float array where the second dimension sums up to 1 for each row\n",
    "        \"\"\"\n",
    "        probability_batches = []\n",
    "        \n",
    "        for batch_x, _ in tqdm.tqdm(data_loader):\n",
    "            current_probabilities = self.predict_probabilities(batch_x).detach().numpy()\n",
    "            probability_batches.append(current_probabilities)\n",
    "\n",
    "        output = np.concatenate(probability_batches, axis=0)\n",
    "        assert isinstance(output, np.ndarray)\n",
    "        assert output.ndim == 2 and output.shape[1] == 10\n",
    "        assert np.allclose(np.sum(output, axis=1), 1.0)\n",
    "        return output\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGLD(SGD):\n",
    "    \"\"\"Implementation of SGLD algorithm.\n",
    "\n",
    "    Paper: \"Bayesian Learning via Stochastic Gradient Langevin Dynamics\", Welling & Teh, 2011.\n",
    "    ----------\n",
    "        \n",
    "    \"\"\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"See `torch.optim.step’.\"\"\"\n",
    "        loss = super().step(closure)\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad_p = p.grad.data\n",
    "                if weight_decay!=0:\n",
    "                    grad_p.add_(alpha=weight_decay,other=p.data)\n",
    "                langevin_noise = torch.randn_like(p.data).mul_(group['lr']**0.5)*0.1 #  use weight 0.1 to balance the noise\n",
    "                p.data.add_(grad_p,alpha=-0.5*group['lr'])\n",
    "                if torch.isnan(p.data).any(): \n",
    "                    exit('Exist NaN param after SGLD, Try to tune the parameter')\n",
    "                if torch.isinf(p.data).any(): \n",
    "                    exit('Exist Inf param after SGLD, Try to tune the parameter')\n",
    "                p.data.add_(langevin_noise)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SGLDTrainer(Framework):\n",
    "    def __init__(self, dataset_train, *args, **kwargs):\n",
    "        super().__init__(dataset_train, *args, **kwargs)\n",
    "\n",
    "        # Hyperparameters and general parameters\n",
    "        self.batch_size = 100\n",
    "        self.learning_rate = 1e-3\n",
    "        self.num_epochs = 5\n",
    "        self.burn_in = 2\n",
    "        self.sample_interval = 3\n",
    "        self.max_size = 10\n",
    "        \n",
    "        self.data_loader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "\n",
    "        # Initialize the SGLD network\n",
    "        self.network = FullyConnectedNN()\n",
    "\n",
    "        # SGLD optimizer is provided\n",
    "        self.optimizer = SGLD(self.network.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Deque to store model samples\n",
    "        self.SGLDSequence = deque()\n",
    "\n",
    "    def train(self):\n",
    "        num_iter = 0\n",
    "        print('Training model')\n",
    "\n",
    "        self.network.train()\n",
    "        progress_bar = trange(self.num_epochs)\n",
    "\n",
    "        for _ in progress_bar:\n",
    "            num_iter += 1\n",
    "\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(self.data_loader):\n",
    "                self.network.zero_grad()\n",
    "\n",
    "                # Perform forward pass\n",
    "                current_logits = self.network(batch_x)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y)\n",
    "\n",
    "                # Backpropagate to get the gradients\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                if batch_idx % self.print_interval == 0:\n",
    "                    current_logits = self.network(batch_x)\n",
    "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
    "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item(), batch=batch_idx)\n",
    "  \n",
    "            # Save the model samples if past the burn-in epochs and reached a regular sampling interval\n",
    "            if num_iter > self.burn_in and num_iter % self.sample_interval == 0:\n",
    "                self.SGLDSequence.append(copy.deepcopy(self.network))\n",
    "                # self.network.state_dict()\n",
    "\n",
    "            # If the deque exceeds the maximum size, delete the oldest model\n",
    "            if len(self.SGLDSequence) > self.max_size:\n",
    "                self.SGLDSequence.popleft()\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #assert x.shape[1] == 28 ** 2\n",
    "        self.network.eval()\n",
    "\n",
    "        # Obtain the prediction from each network in SGLDSequence and combine the predictions\n",
    "        estimated_probability = torch.zeros((10000, 10))\n",
    "        for model in self.SGLDSequence:\n",
    "\n",
    "\n",
    "            self.network.load_state_dict(model.state_dict())\n",
    "            logits = self.network(x).detach()\n",
    "            estimated_probability += F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Normalize the combined predictions\n",
    "        estimated_probability /= len(self.SGLDSequence)\n",
    "\n",
    "        assert estimated_probability.shape == (x.shape[0], 10)  \n",
    "        return estimated_probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.18s/it, acc=0.109, batch=0, loss=2.3]  \n"
     ]
    }
   ],
   "source": [
    "lol = SGLDTrainer(sub_train_set)\n",
    "lol.train()\n",
    "\n",
    "# lol.predict_probabilities(sub_test_set.dataset.data.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT Code LOL\n",
    "# Initialize variables to store the number of correct and total predictions\n",
    "num_correct = 0\n",
    "num_total = 0\n",
    "class_probs = lol.predict_probabilities(sub_test_set.dataset.data.float())\n",
    "\n",
    "# Iterate over the samples in the test set\n",
    "for sample, true_class in sub_test_set:\n",
    "    # Get the estimated probabilities for each class from the Bayesian Neural Network\n",
    "    class_probs_1 = class_probs[num_total]\n",
    "    \n",
    "    # Choose the class with the highest probability as the predicted class\n",
    "    predicted_class = class_probs_1.argmax()\n",
    "    \n",
    "    # Update the count of correct and total predictions\n",
    "    if predicted_class == true_class:\n",
    "        num_correct += 1\n",
    "    num_total += 1\n",
    "\n",
    "# Calculate the accuracy as the number of correct predictions divided by the total number of predictions\n",
    "\n",
    "accuracy = num_correct / num_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.109"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lol.predict_probabilities(sub_test_set.dataset.data.float())\n",
    "\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f2fa85e4cfd485d54f024c1b8a04817ed39643e0edc5d72a4881a916b82e72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
