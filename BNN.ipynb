{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.distributions import Categorical, Normal, StudentT\n",
    "import torch.distributions as dist\n",
    "from torch.optim import SGD\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "import tqdm\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "transform = transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample from trainset\n",
    "n_subsamples_train = 5000 # size of subset\n",
    "sub_train_idx = random.sample(range(60000),n_subsamples_train) # 60'000 is train size in MNSIT\n",
    "sub_train_set = Subset(train_set, sub_train_idx)\n",
    "\n",
    "# subsample from testset\n",
    "n_subsamples_test = 1000  # size of subset\n",
    "sub_test_idx = random.sample(range(10000), n_subsamples_test) # 10'000 is test size in MNIST\n",
    "sub_test_set = Subset(test_set, sub_test_idx)\n",
    "\n",
    "# load\n",
    "#sub_train_dataloader = DataLoader(sub_train_set, batch_size=64, shuffle=True)\n",
    "sub_test_dataloader = DataLoader(sub_test_set, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick Check \n",
    "feature_test = sub_test_set[999][0]\n",
    "label_test = sub_test_set[999][1]\n",
    "print(label_test)\n",
    "plt.imshow(feature_test.squeeze(), cmap='hot')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "feature_train = sub_train_set[1999][0]\n",
    "label_train = sub_train_set[1999][1]\n",
    "print(label_train)\n",
    "plt.imshow(feature_train.squeeze(), cmap='hot')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Model Part Krause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Class for Priors\n",
    "\n",
    "class Prior:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def sample(self,n):\n",
    "        pass\n",
    "    def log_likelihood(self,values):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Prior\n",
    "# Change: to a subclass of Prior\n",
    "\n",
    "class IsotropicGaussian(Prior):\n",
    "    def __init__(self, mean=0, std=1):\n",
    "        super(IsotropicGaussian,self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def sample(self, n):\n",
    "        return np.random.normal(self.mean, self.std, size=n)\n",
    "\n",
    "    def log_likelihood(self, weights):\n",
    "        return Normal(self.mean, self.std).log_prob(torch.tensor(weights)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Neural Network \n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, in_features = 28*28, out_features = 10, hidden_units = 100, hidden_layers = 3):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(in_features, hidden_units))\n",
    "        for i in range(hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_units, hidden_units))\n",
    "        self.output_layer = nn.Linear(hidden_units, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,28*28)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        class_probs = self.output_layer(x)\n",
    "        return class_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network \n",
    "\n",
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self, in_features = 28*28, out_features = 10):\n",
    "\n",
    "    # define two convolutional layers with 64 channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=2000, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # define two max-pooling layers\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # define a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=3*3*64, out_features=out_features)\n",
    "        \n",
    "        # define the ReLU nonlinearity\n",
    "        self.activ = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activ(self.maxpool1(self.activ(self.conv1(x))))\n",
    "        x = self.activ(self.maxpool2(self.activ(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        class_probs = self.fc(x)\n",
    "        return class_probs\n",
    "\n",
    "# DEBUG THIS \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework\n",
    "\n",
    "class Framework(object):\n",
    "    def __init__(self, training_set, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Basic Framework for your bayesian neural network.\n",
    "        SGLD will be based on this.\n",
    "        \"\"\"\n",
    "        self.train_set = training_set\n",
    "        self.print_interval = 64 # number of batches until updated metrics are displayed during training\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict(self, data_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the class probabilities using your trained model.\n",
    "        This method should return an (num_samples, 10) NumPy float array\n",
    "        such that the second dimension sums up to 1 for each row.\n",
    "\n",
    "        :param data_loader: Data loader yielding the samples to predict on\n",
    "        :return: (num_samples, 10) NumPy float array where the second dimension sums up to 1 for each row\n",
    "        \"\"\"\n",
    "        probability_batches = []\n",
    "        \n",
    "        for batch_x, _ in tqdm.tqdm(data_loader):\n",
    "            current_probabilities = self.predict_probabilities(batch_x).detach().numpy()\n",
    "            probability_batches.append(current_probabilities)\n",
    "\n",
    "        output = np.concatenate(probability_batches, axis=0)\n",
    "        assert isinstance(output, np.ndarray)\n",
    "        assert output.ndim == 2 and output.shape[1] == 10\n",
    "        assert np.allclose(np.sum(output, axis=1), 1.0)\n",
    "        return output\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from typing import Sequence, Optional, Callable, Tuple, Dict, Union\n",
    "import typing\n",
    "\n",
    "\n",
    "def dot(a, b):\n",
    "    \"return (a*b).sum().item()\"\n",
    "    return (a.view(-1) @ b.view(-1)).item()\n",
    "\n",
    "## Fortuin SGLD code\n",
    "\n",
    "\n",
    "class SGLD(torch.optim.Optimizer):\n",
    "    \"\"\"SGLD with momentum, preconditioning and diagnostics from Wenzel et al. 2020.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate\n",
    "        num_data (int): the number of data points in this learning task\n",
    "        momentum (float): momentum factor (default: 0)\n",
    "        temperature (float): Temperature for tempering the posterior.\n",
    "                             temperature=0 corresponds to SGD with momentum.\n",
    "        rmsprop_alpha: decay for the moving average of the squared gradients\n",
    "        rmsprop_eps: the regularizer parameter for the RMSProp update\n",
    "        raise_on_no_grad (bool): whether to complain if a parameter does not\n",
    "                                 have a gradient\n",
    "        raise_on_nan: whether to complain if a gradient is not all finite.\n",
    "    \"\"\"\n",
    "    def __init__(self, params: Sequence[Union[torch.nn.Parameter, Dict]], lr: float,\n",
    "                 num_data: int, momentum: float=0, temperature: float=1.,\n",
    "                 rmsprop_alpha: float=0.99, rmsprop_eps: float=1e-8,  # Wenzel et al. use 1e-7\n",
    "                 raise_on_no_grad: bool=True, raise_on_nan: bool=False):\n",
    "        assert lr >= 0 and num_data >= 0 and momentum >= 0 and temperature >= 0\n",
    "        defaults = dict(lr=lr, num_data=num_data, momentum=momentum,\n",
    "                        rmsprop_alpha=rmsprop_alpha, rmsprop_eps=rmsprop_eps,\n",
    "                        temperature=temperature)\n",
    "        super(SGLD, self).__init__(params, defaults)\n",
    "        self.raise_on_no_grad = raise_on_no_grad\n",
    "        self.raise_on_nan = raise_on_nan\n",
    "        # OK to call this one, but not `sample_momentum`, because\n",
    "        # `update_preconditioner` uses no random numbers.\n",
    "        self.update_preconditioner()\n",
    "        self._step_count = 0  # keep the `torch.optim.scheduler` happy\n",
    "\n",
    "    def _preconditioner_default(self, state, p) -> float:\n",
    "        try:\n",
    "            return state['preconditioner']\n",
    "        except KeyError:\n",
    "            v = state['preconditioner'] = 1.\n",
    "            return v\n",
    "\n",
    "    def delta_energy(self, a, b) -> float:\n",
    "        return math.inf\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_momentum(self, keep=0.0):\n",
    "        \"Sample the momenta for all the parameters\"\n",
    "        assert 0 <= keep and keep <= 1.\n",
    "        if keep == 1.:\n",
    "            return\n",
    "        for group in self.param_groups:\n",
    "            std = math.sqrt(group['temperature']*(1-keep))\n",
    "            for p in group['params']:\n",
    "                if keep == 0.0:\n",
    "                    self.state[p]['momentum_buffer'] = torch.randn_like(p).mul_(std)\n",
    "                else:\n",
    "                    self.state[p]['momentum_buffer'].mul_(math.sqrt(keep)).add_(torch.randn_like(p), alpha=std)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Optional[Callable[..., torch.Tensor]]=None,\n",
    "             calc_metrics=True, save_state=False):\n",
    "        assert save_state is False\n",
    "        return self._step_internal(self._update_group_fn, self._step_fn,\n",
    "                                   closure, calc_metrics=calc_metrics)\n",
    "    initial_step = step\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def final_step(self, closure: Optional[Callable[..., torch.Tensor]]=None,\n",
    "                   calc_metrics=True, save_state=False):\n",
    "        assert save_state is False\n",
    "        return self._step_internal(self._update_group_fn, self._step_fn,\n",
    "                                   closure, calc_metrics=calc_metrics,\n",
    "                                   is_final=True)\n",
    "\n",
    "\n",
    "    def _step_internal(self, update_group_fn, step_fn, closure, **step_fn_kwargs):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        try:\n",
    "            for group in self.param_groups:\n",
    "                update_group_fn(group)\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None:\n",
    "                        if self.raise_on_no_grad:\n",
    "                            raise RuntimeError(\n",
    "                                f\"No gradient for parameter with shape {p.shape}\")\n",
    "                        continue\n",
    "                    if self.raise_on_nan and not torch.isfinite(p.grad).all():\n",
    "                        raise ValueError(\n",
    "                            f\"Gradient of shape {p.shape} is not finite: {p.grad}\")\n",
    "                    step_fn(group, p, self.state[p], **step_fn_kwargs)\n",
    "\n",
    "        except KeyError as e:\n",
    "            if e.args[0] == \"momentum_buffer\":\n",
    "                raise RuntimeError(\"No 'momentum_buffer' stored in state. \"\n",
    "                                   \"Perhaps you forgot to call `sample_momentum`?\")\n",
    "            raise e\n",
    "        return loss\n",
    "\n",
    "    def _update_group_fn(self, g):\n",
    "        g['hn'] = math.sqrt(g['lr'] * g['num_data'])\n",
    "        g['h'] = math.sqrt(g['lr'] / g['num_data'])\n",
    "        g['noise_std'] = math.sqrt(2*(1 - g['momentum']) * g['temperature'])\n",
    "\n",
    "    def _step_fn(self, group, p, state, calc_metrics=True, is_final=False):\n",
    "        \"\"\"if is_final, do not change parameters or momentum\"\"\"\n",
    "        M_rsqrt = self._preconditioner_default(state, p)\n",
    "        d = p.numel()\n",
    "\n",
    "        # Update the momentum with the gradient\n",
    "        if group['momentum'] > 0:\n",
    "            momentum = state['momentum_buffer']\n",
    "            if calc_metrics:\n",
    "                # NOTE: the momentum is from the previous time step\n",
    "                state['est_temperature'] = dot(momentum, momentum) / d\n",
    "            if not is_final:\n",
    "                momentum.mul_(group['momentum']).add_(p.grad, alpha=-group['hn']*M_rsqrt)\n",
    "        else:\n",
    "            if not is_final:\n",
    "                momentum = p.grad.detach().mul(-group['hn']*M_rsqrt)\n",
    "            if calc_metrics:\n",
    "                # TODO: make the momentum be from the previous time step\n",
    "                state['est_temperature'] = dot(momentum, momentum) / d\n",
    "\n",
    "        if not is_final:\n",
    "            # Add noise to momentum\n",
    "            if group['temperature'] > 0:\n",
    "                momentum.add_(torch.randn_like(momentum), alpha=group['noise_std'])\n",
    "\n",
    "        if calc_metrics:\n",
    "            # NOTE: p and p.grad are from the same time step\n",
    "            state['est_config_temp'] = dot(p, p.grad) * (group['num_data']/d)\n",
    "\n",
    "        if not is_final:\n",
    "            # Take the gradient step\n",
    "            p.add_(momentum, alpha=group['h']*M_rsqrt)\n",
    "\n",
    "            # RMSProp moving average\n",
    "            alpha = group['rmsprop_alpha']\n",
    "            state['square_avg'].mul_(alpha).addcmul_(p.grad, p.grad, value=1 - alpha)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_preconditioner(self):\n",
    "        \"\"\"Updates the preconditioner for each parameter `state['preconditioner']` using\n",
    "        the estimated `state['square_avg']`.\n",
    "        \"\"\"\n",
    "        precond = OrderedDict()\n",
    "        min_s = math.inf\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            eps = group['rmsprop_eps']\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                try:\n",
    "                    square_avg = state['square_avg']\n",
    "                except KeyError:\n",
    "                    square_avg = state['square_avg'] = torch.ones_like(p)\n",
    "\n",
    "                precond[p] = square_avg.mean().item() + eps\n",
    "                min_s = min(min_s, precond[p])\n",
    "\n",
    "        for p, new_M in precond.items():\n",
    "            # ^(1/2) to form the preconditioner,\n",
    "            # ^(-1/2) because we want the preconditioner's inverse square root.\n",
    "            self.state[p]['preconditioner'] = (new_M / min_s)**(-1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGLDTrainer(Framework):\n",
    "    def __init__(self, dataset_train, network, prior, *args, **kwargs):\n",
    "        super().__init__(dataset_train, *args, **kwargs)\n",
    "\n",
    "        # Hyperparameters and general parameters\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 1e-3\n",
    "        self.num_epochs = 100\n",
    "        self.burn_in = 50\n",
    "        self.sample_interval = 2\n",
    "        self.max_size = 100\n",
    "        \n",
    "        self.sample_size = dataset_train.__len__()\n",
    "        self.data_loader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "\n",
    "        # Set Prior\n",
    "        self.prior = prior\n",
    "\n",
    "        # Initialize the SGLD network\n",
    "        self.network = network\n",
    "\n",
    "        # SGLD optimizer is provided\n",
    "        self.optimizer = SGLD(self.network.parameters(), lr=self.learning_rate, num_data=self.sample_size)\n",
    "\n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "        # Deque to store model samples\n",
    "        self.SGLDSequence = deque()\n",
    "\n",
    "    def train(self):\n",
    "        num_iter = 0\n",
    "        print('Training model')\n",
    "\n",
    "        self.network.train()\n",
    "        progress_bar = trange(self.num_epochs)\n",
    "\n",
    "        for _ in progress_bar:\n",
    "            num_iter += 1\n",
    "\n",
    "            N = self.sample_size\n",
    "\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(self.data_loader):\n",
    "                self.network.zero_grad()\n",
    "                n = len(batch_x)\n",
    "\n",
    "                # Perform forward pass\n",
    "                current_logits = self.network(batch_x)\n",
    "\n",
    "                # Calculate log_likelihood of weights for a given prior\n",
    "\n",
    "                parameters = self.network.state_dict()     # extract weights from network\n",
    "                param_values = list(parameters.values())    # list weights\n",
    "                param_flat = np.concatenate([v.flatten() for v in param_values])    # flattern\n",
    "                log_likelihood = self.prior.log_likelihood(param_flat)              # calculate log_lik\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y, reduction = \"sum\") - log_likelihood*n/N\n",
    "\n",
    "                # Backpropagate to get the gradients\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                if batch_idx % self.print_interval == 0:\n",
    "                    current_logits = self.network(batch_x)\n",
    "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
    "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item(),\n",
    "                    nll_loss= F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y, reduction = \"sum\").item(),\n",
    "                    log_lik_prior = - log_likelihood.item()*n/N)\n",
    "\n",
    "            # Update the learning rate\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Save the model samples if past the burn-in epochs and reached a regular sampling interval\n",
    "            if num_iter > self.burn_in and num_iter % self.sample_interval == 0:\n",
    "                self.SGLDSequence.append(copy.deepcopy(self.network))\n",
    "                # self.network.state_dict()\n",
    "\n",
    "            # If the deque exceeds the maximum size, delete the oldest model\n",
    "            if len(self.SGLDSequence) > self.max_size:\n",
    "                self.SGLDSequence.popleft()\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #assert x.shape[1] == 28 ** 2\n",
    "        self.network.eval()\n",
    "\n",
    "        # Obtain the prediction from each network in SGLDSequence and combine the predictions\n",
    "        estimated_probability = torch.zeros((len(x), 10))\n",
    "        for model in self.SGLDSequence:\n",
    "\n",
    "\n",
    "            self.network.load_state_dict(model.state_dict())\n",
    "            logits = self.network(x).detach()\n",
    "            estimated_probability += F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Normalize the combined predictions\n",
    "        estimated_probability /= len(self.SGLDSequence)\n",
    "\n",
    "        assert estimated_probability.shape == (x.shape[0], 10)  \n",
    "        return estimated_probability\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [24:16<00:00, 14.56s/it, acc=1, log_lik_prior=588, loss=588, nll_loss=0.679]   \n"
     ]
    }
   ],
   "source": [
    "lol = SGLDTrainer(train_set,network= FullyConnectedNN(),prior= IsotropicGaussian(mean = 0, std = 100))\n",
    "lol.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9710\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "x_test = sub_test_set.dataset.data.float()\n",
    "y_test = sub_test_set.dataset.targets\n",
    "\n",
    "# predicted probabilities\n",
    "class_probs = lol.predict_probabilities(x_test)\n",
    "\n",
    "# accuracy\n",
    "accuracy = (class_probs.argmax(axis=1) == y_test).float().mean()\n",
    "print(f'Test Accuracy: {accuracy.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration Error: 0.0287\n"
     ]
    }
   ],
   "source": [
    "# Calibration\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import calibration_error\n",
    "\n",
    "calib_err = calibration_error(class_probs, y_test, n_bins = 30, task = \"multiclass\", norm=\"l1\", num_classes=10)\n",
    "print(f'Calibration Error: {calib_err.item():.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student-t prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentTPrior(Prior):\n",
    "    \"\"\"\n",
    "    Student-T Prior\n",
    "    \"\"\"\n",
    "    def __init__(self, df=10, loc=0, scale=1, Temperature: float= 1.0):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "        self.Temperature = Temperature\n",
    "\n",
    "    def log_likelihood(self, values) -> torch.Tensor:\n",
    "        return StudentT(self.df, self.loc, self.scale).log_prob(torch.tensor(values)).sum() / self.Temperature\n",
    "\n",
    "    def sample(self,n):\n",
    "        return StudentT(self.df, self.loc, self.scale).sample((n,))  # sample from student-T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [22:41<00:00, 13.62s/it, acc=0.984, log_lik_prior=101, loss=104, nll_loss=3.85]\n"
     ]
    }
   ],
   "source": [
    "student_lol = SGLDTrainer(train_set,network = FullyConnectedNN(), prior=StudentTPrior())\n",
    "student_lol.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9703\n"
     ]
    }
   ],
   "source": [
    "# predicted probabilities\n",
    "class_probs_t = student_lol.predict_probabilities(x_test)\n",
    "\n",
    "# accuracy\n",
    "accuracy = (class_probs_t.argmax(axis=1) == y_test).float().mean()\n",
    "print(f'Test Accuracy: {accuracy.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration Error: 0.0295\n"
     ]
    }
   ],
   "source": [
    "calib_err = calibration_error(class_probs_t, y_test, n_bins = 30, task = \"multiclass\", norm=\"l1\", num_classes=10)\n",
    "print(f'Calibration Error: {calib_err.item():.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplacePrior(Prior):\n",
    "    \"\"\"\n",
    "    Laplace Prior\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor, Temperature: float=1.0):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.sig = torch.log(1 + torch.exp(rho))   # transform rho\n",
    "        self.Temperature = Temperature\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        return dist.Laplace(self.mu, self.sig).log_prob(torch.tensor(values)).sum() / self.Temperature\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        return dist.Laplace(self.mu, self.sig).sample(torch.Tensor(1).shape).view(self.mu.shape)  # sample from laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [23:57<00:00, 14.38s/it, acc=1, log_lik_prior=106, loss=107, nll_loss=1.1]     \n"
     ]
    }
   ],
   "source": [
    "Laplace_lol = SGLDTrainer(train_set,network = FullyConnectedNN(), prior=LaplacePrior(mu=torch.tensor(0.), rho=torch.tensor(1.)))\n",
    "Laplace_lol.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9687\n",
      "Calibration Error: 0.0309\n"
     ]
    }
   ],
   "source": [
    "# predicted probabilities\n",
    "class_probs_t = Laplace_lol.predict_probabilities(x_test)\n",
    "\n",
    "# accuracy\n",
    "accuracy = (class_probs_t.argmax(axis=1) == y_test).float().mean()\n",
    "print(f'Test Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "calib_err = calibration_error(class_probs_t, y_test, n_bins = 30, task = \"multiclass\", norm=\"l1\", num_classes=10)\n",
    "print(f'Calibration Error: {calib_err.item():.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Gamma prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "\n",
    "class InverseGamma(Prior):\n",
    "    \"\"\" Inverse Gamma distribution \"\"\"\n",
    "    def __init__(self, shape: torch.Tensor, rate: torch.Tensor, Temperature: float = 1.0):\n",
    "        \"\"\"\n",
    "        shape: shape parameters of the distribution\n",
    "        rate: rate parameters of the distribution\n",
    "        \"\"\"\n",
    "        self.shape = shape\n",
    "        self.rate = rate\n",
    "        self.Temperature = Temperature\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the value of the predictive log likelihood at the target value\n",
    "        Args:\n",
    "            target: Torch tensor of floats, point(s) to evaluate the logprob\n",
    "        Returns:\n",
    "            loglike: float, the log likelihood\n",
    "        \"\"\"\n",
    "        x = (self.rate**self.shape) / gamma(self.shape)\n",
    "        y = torch.tensor(values)**(-self.shape - 1)\n",
    "        z = torch.exp(-self.rate / values)\n",
    "        return torch.log(x * y * z)\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        # sample from gamma and return 1/x\n",
    "        x = dist.Gamma(self.shape, self.rate).sample()\n",
    "        return 1/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IG_lol = SGLDTrainer(sub_train_set,network = FullyConnectedNN(), prior=InverseGamma(shape=torch.tensor(0.5), rate=torch.tensor(0.5)))\n",
    "IG_lol.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities\n",
    "class_probs_t = IG_lol.predict_probabilities(x_test)\n",
    "\n",
    "# accuracy\n",
    "accuracy = (class_probs_t.argmax(axis=1) == y_test).float().mean()\n",
    "print(f'Test Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "calib_err = calibration_error(class_probs_t, y_test, n_bins = 30, task = \"multiclass\", norm=\"l1\", num_classes=10)\n",
    "print(f'Calibration Error: {calib_err.item():.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixture(Prior):\n",
    "    \"\"\"\n",
    "    Mixture of 2 gaussians with same mean but different variances\n",
    "    \"\"\"\n",
    "    def __init__(self,  mu: torch.Tensor, rho1: torch.Tensor, rho2: torch.Tensor, mixing_coef: float=0.7 ,Temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.rho1 = rho1\n",
    "        self.rho2 = rho2\n",
    "        self.sig1 = torch.log(1 + torch.exp(rho1))  # transform rho\n",
    "        self.sig2 = torch.log(1 + torch.exp(rho2))  # transform rho\n",
    "        self.mixing_coef = mixing_coef\n",
    "        self.Temperature = Temperature\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        p1 = dist.Normal(self.mu, self.sig1).log_prob(values)\n",
    "        p2 = dist.Normal(self.mu, self.sig2).log_prob(values)\n",
    "        log_lik = (p1 * self.mixing_coef + p2 * (1-self.mixing_coef)).sum() / self.Temperature\n",
    "        return log_lik\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        eps = torch.randn_like(self.mu)\n",
    "        sample1 = self.mu + self.sig1 * eps\n",
    "        eps = torch.randn_like(self.mu)\n",
    "        sample2 = self.mu + self.sig2 * eps\n",
    "        return sample1 * self.mixing_coef + sample2 * (1-self.mixing_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [24:13<00:00, 14.54s/it, acc=1, log_lik_prior=106, loss=106, nll_loss=0.348]   \n"
     ]
    }
   ],
   "source": [
    "GMM_lol = SGLDTrainer(train_set,network = FullyConnectedNN(), prior=LaplacePrior(mu=torch.tensor(0.), rho=torch.tensor(1.)))\n",
    "GMM_lol.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9665\n",
      "Calibration Error: 0.0332\n"
     ]
    }
   ],
   "source": [
    "# predicted probabilities\n",
    "class_probs_t = GMM_lol.predict_probabilities(x_test)\n",
    "\n",
    "# accuracy\n",
    "accuracy = (class_probs_t.argmax(axis=1) == y_test).float().mean()\n",
    "print(f'Test Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "calib_err = calibration_error(class_probs_t, y_test, n_bins = 30, task = \"multiclass\", norm=\"l1\", num_classes=10)\n",
    "print(f'Calibration Error: {calib_err.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fortuin SGLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27f3840d2a8e92b9098d5d1f2625c1fbd9e0739b6ace23f817bf68ad3a8d77b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
