{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.distributions import Categorical, Normal, StudentT\n",
    "from torch.optim import SGD\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "import tqdm\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "transform = transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample from trainset\n",
    "n_subsamples_train = 2000 # size of subset\n",
    "sub_train_idx = random.sample(range(60000),n_subsamples_train) # 60'000 is train size in MNSIT\n",
    "sub_train_set = Subset(train_set, sub_train_idx)\n",
    "\n",
    "# subsample from testset\n",
    "n_subsamples_test = 1000  # size of subset\n",
    "sub_test_idx = random.sample(range(10000), n_subsamples_test) # 10'000 is test size in MNIST\n",
    "sub_test_set = Subset(test_set, sub_test_idx)\n",
    "\n",
    "# load\n",
    "#sub_train_dataloader = DataLoader(sub_train_set, batch_size=64, shuffle=True)\n",
    "sub_test_dataloader = DataLoader(sub_test_set, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Check \n",
    "feature_test = sub_test_set[999][0]\n",
    "label_test = sub_test_set[999][1]\n",
    "print(label_test)\n",
    "plt.imshow(feature_test.squeeze(), cmap='hot')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "feature_train = sub_train_set[1999][0]\n",
    "label_train = sub_train_set[1999][1]\n",
    "print(label_train)\n",
    "plt.imshow(feature_train.squeeze(), cmap='hot')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Model Part Krause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Class for Priors\n",
    "\n",
    "class Prior:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def sample(self,n):\n",
    "        pass\n",
    "    def log_likelihood(self,values):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Prior\n",
    "# Change: to a subclass of Prior\n",
    "\n",
    "class IsotropicGaussian(Prior):\n",
    "    def __init__(self, mean=0, std=1):\n",
    "        super(IsotropicGaussian,self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def sample(self, n):\n",
    "        return np.random.normal(self.mean, self.std, size=n)\n",
    "\n",
    "    def log_likelihood(self, weights):\n",
    "        return Normal(self.mean, self.std).log_prob(torch.tensor(weights)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Neural Network \n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, in_features = 28*28, out_features = 10, hidden_units = 100, hidden_layers = 3):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(in_features, hidden_units))\n",
    "        for i in range(hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_units, hidden_units))\n",
    "        self.output_layer = nn.Linear(hidden_units, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,28*28)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        class_probs = self.output_layer(x)\n",
    "        return class_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network \n",
    "\n",
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self, in_features = 28*28, out_features = 10):\n",
    "\n",
    "    # define two convolutional layers with 64 channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=2000, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # define two max-pooling layers\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # define a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=3*3*64, out_features=out_features)\n",
    "        \n",
    "        # define the ReLU nonlinearity\n",
    "        self.activ = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activ(self.maxpool1(self.activ(self.conv1(x))))\n",
    "        x = self.activ(self.maxpool2(self.activ(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        class_probs = self.fc(x)\n",
    "        return class_probs\n",
    "\n",
    "# DEBUG THIS \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework\n",
    "\n",
    "class Framework(object):\n",
    "    def __init__(self, training_set, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Basic Framework for your bayesian neural network.\n",
    "        SGLD will be based on this.\n",
    "        \"\"\"\n",
    "        self.train_set = training_set\n",
    "        self.print_interval = 64 # number of batches until updated metrics are displayed during training\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict(self, data_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the class probabilities using your trained model.\n",
    "        This method should return an (num_samples, 10) NumPy float array\n",
    "        such that the second dimension sums up to 1 for each row.\n",
    "\n",
    "        :param data_loader: Data loader yielding the samples to predict on\n",
    "        :return: (num_samples, 10) NumPy float array where the second dimension sums up to 1 for each row\n",
    "        \"\"\"\n",
    "        probability_batches = []\n",
    "        \n",
    "        for batch_x, _ in tqdm.tqdm(data_loader):\n",
    "            current_probabilities = self.predict_probabilities(batch_x).detach().numpy()\n",
    "            probability_batches.append(current_probabilities)\n",
    "\n",
    "        output = np.concatenate(probability_batches, axis=0)\n",
    "        assert isinstance(output, np.ndarray)\n",
    "        assert output.ndim == 2 and output.shape[1] == 10\n",
    "        assert np.allclose(np.sum(output, axis=1), 1.0)\n",
    "        return output\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGLD(SGD):\n",
    "    \"\"\"Implementation of SGLD algorithm.\n",
    "\n",
    "    Paper: \"Bayesian Learning via Stochastic Gradient Langevin Dynamics\", Welling & Teh, 2011.\n",
    "    ----------\n",
    "        \n",
    "    \"\"\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"See `torch.optim.stepâ€™.\"\"\"\n",
    "        loss = super().step(closure)\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad_p = p.grad.data\n",
    "                #log_prior_weights = prior.log_likelihood(p.data)    # LOG_LIKELIHOOD OF WEIGHTS WITH PRIOR DISTRIBUTION\n",
    "                #cost_and_regular = grad_p + log_prior_weights      # SUM OF THE TWO\n",
    "                if weight_decay!=0:\n",
    "                    grad_p.add_(alpha=weight_decay,other=p.data)\n",
    "                langevin_noise = torch.randn_like(p.data).mul_(group['lr']**0.5)*0.1 #  use weight 0.1 to balance the noise\n",
    "                p.data.add_(grad_p,alpha=-0.5*group['lr'])\n",
    "                #p.data.add_(cost_and_regular,alpha=-0.5*group['lr'])\n",
    "                if torch.isnan(p.data).any(): \n",
    "                    exit('Exist NaN param after SGLD, Try to tune the parameter')\n",
    "                if torch.isinf(p.data).any(): \n",
    "                    exit('Exist Inf param after SGLD, Try to tune the parameter')\n",
    "                p.data.add_(langevin_noise)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# LOOK AT SKRIPT PAI AT PAGE 130/118 FOR THE ALGORITHM TO IMPLEMENT IT BY YOURSELF #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGLDTrainer(Framework):\n",
    "    def __init__(self, dataset_train, network, prior, *args, **kwargs):\n",
    "        super().__init__(dataset_train, *args, **kwargs)\n",
    "\n",
    "        # Hyperparameters and general parameters\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 1e-3\n",
    "        self.num_epochs = 200\n",
    "        self.burn_in = 100\n",
    "        self.sample_interval = 2\n",
    "        self.max_size = 100\n",
    "        \n",
    "        self.data_loader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "\n",
    "        # Set Prior\n",
    "        self.prior = prior\n",
    "\n",
    "        # Initialize the SGLD network\n",
    "        self.network = network\n",
    "\n",
    "        # SGLD optimizer is provided\n",
    "        self.optimizer = SGLD(self.network.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Deque to store model samples\n",
    "        self.SGLDSequence = deque()\n",
    "\n",
    "    def train(self):\n",
    "        num_iter = 0\n",
    "        print('Training model')\n",
    "\n",
    "        self.network.train()\n",
    "        progress_bar = trange(self.num_epochs)\n",
    "\n",
    "        for _ in progress_bar:\n",
    "            num_iter += 1\n",
    "\n",
    "            N = len(self.data_loader)\n",
    "\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(self.data_loader):\n",
    "                self.network.zero_grad()\n",
    "                n = len(batch_x)\n",
    "\n",
    "                # Perform forward pass\n",
    "                current_logits = self.network(batch_x)\n",
    "\n",
    "                # Calculate log_likelihood of weights for a given prior\n",
    "\n",
    "                parameters = self.network.state_dict()     # extract weights from network\n",
    "                param_values = list(parameters.values())    # list weights\n",
    "                param_flat = np.concatenate([v.flatten() for v in param_values])    # flattern\n",
    "                log_likelihood = self.prior.log_likelihood(param_flat)              # calculate log_lik\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = N/n*F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y) - log_likelihood/len(param_flat)\n",
    "\n",
    "                if batch_idx % self.print_interval == 0:\n",
    "                    current_logits = self.network(batch_x)\n",
    "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
    "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item(),\n",
    "                    nll_loss=N/n*F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y).item(),\n",
    "                    log_lik_prior = - log_likelihood.item()/len(param_flat))\n",
    "\n",
    "                # Backpropagate to get the gradients\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "            \n",
    "            # Save the model samples if past the burn-in epochs and reached a regular sampling interval\n",
    "            if num_iter > self.burn_in and num_iter % self.sample_interval == 0:\n",
    "                self.SGLDSequence.append(copy.deepcopy(self.network))\n",
    "                # self.network.state_dict()\n",
    "\n",
    "            # If the deque exceeds the maximum size, delete the oldest model\n",
    "            if len(self.SGLDSequence) > self.max_size:\n",
    "                self.SGLDSequence.popleft()\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #assert x.shape[1] == 28 ** 2\n",
    "        self.network.eval()\n",
    "\n",
    "        # Obtain the prediction from each network in SGLDSequence and combine the predictions\n",
    "        estimated_probability = torch.zeros((len(x), 10))\n",
    "        for model in self.SGLDSequence:\n",
    "\n",
    "\n",
    "            self.network.load_state_dict(model.state_dict())\n",
    "            logits = self.network(x).detach()\n",
    "            estimated_probability += F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Normalize the combined predictions\n",
    "        estimated_probability /= len(self.SGLDSequence)\n",
    "\n",
    "        assert estimated_probability.shape == (x.shape[0], 10)  \n",
    "        return estimated_probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = SGLDTrainer(sub_train_set,network= FullyConnectedNN(),prior= IsotropicGaussian())\n",
    "lol.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "x_test = sub_test_set.dataset.data.float()\n",
    "y_test = sub_test_set.dataset.targets\n",
    "\n",
    "# predicted probabilities\n",
    "class_probs = lol.predict_probabilities(x_test)\n",
    "\n",
    "# accuracy\n",
    "accuracy = (class_probs.argmax(axis=1) == y_test).float().mean()\n",
    "print(f'Test Accuracy: {accuracy.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import calibration_error\n",
    "\n",
    "calib_err = calibration_error(class_probs, y_test, n_bins = 30, task = \"multiclass\", norm=\"l1\", num_classes=10)\n",
    "print(f'Calibration Error: {calib_err.item():.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with StudentT Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentTPrior(Prior):\n",
    "    \"\"\"\n",
    "    Student-T Prior\n",
    "    \"\"\"\n",
    "    def __init__(self, df=10, loc=0, scale=1, Temperature: float= 1.0):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "        self.Temperature = Temperature\n",
    "\n",
    "    def log_likelihood(self, values) -> torch.Tensor:\n",
    "        return StudentT(self.df, self.loc, self.scale).log_prob(torch.tensor(values)).sum() / self.Temperature\n",
    "\n",
    "    def sample(self,n):\n",
    "        return StudentT(self.df, self.loc, self.scale).sample((n,))  # sample from student-T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_lol = SGLDTrainer(sub_train_set,network = FullyConnectedNN(), prior=StudentTPrior())\n",
    "student_lol.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities\n",
    "class_probs_t = student_lol.predict_probabilities(x_test)\n",
    "\n",
    "# accuracy\n",
    "accuracy = (class_probs_t.argmax(axis=1) == y_test).float().mean()\n",
    "print(f'Test Accuracy: {accuracy.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_err = calibration_error(class_probs_t, y_test, n_bins = 30, task = \"multiclass\", norm=\"l1\", num_classes=10)\n",
    "print(f'Calibration Error: {calib_err.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f2fa85e4cfd485d54f024c1b8a04817ed39643e0edc5d72a4881a916b82e72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
