{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import Subset\n",
    "from torch.distributions import Categorical, Normal, StudentT\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import calibration_error\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque, OrderedDict\n",
    "from tqdm import trange\n",
    "import tqdm\n",
    "import copy\n",
    "import typing\n",
    "from typing import Sequence, Optional, Callable, Tuple, Dict, Union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "transform = transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSAMPLE IF NEEDED\n",
    "\n",
    "# subsample from trainset\n",
    "n_subsamples_train = 2000 # size of subset\n",
    "sub_train_idx = random.sample(range(60000),n_subsamples_train)\n",
    "sub_train_set = Subset(train_set, sub_train_idx)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework for Priors\n",
    "\n",
    "class Prior:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sample(self,n):\n",
    "        pass\n",
    "\n",
    "    def log_likelihood(self,values):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Prior\n",
    "\n",
    "class IsotropicGaussian(Prior):\n",
    "    def __init__(self, mean=0, std=1):\n",
    "        super(IsotropicGaussian,self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def sample(self, n):\n",
    "        return np.random.normal(self.mean, self.std, size=n)\n",
    "\n",
    "    def log_likelihood(self, weights):\n",
    "        return Normal(self.mean, self.std).log_prob(torch.tensor(weights)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StudentT Prior\n",
    "\n",
    "class StudentTPrior(Prior):\n",
    "    \"\"\"\n",
    "    Student-T Prior\n",
    "    \"\"\"\n",
    "    def __init__(self, df=10, loc=0, scale=1, Temperature: float= 1.0):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "        self.Temperature = Temperature\n",
    "\n",
    "    def log_likelihood(self, values) -> torch.Tensor:\n",
    "        return StudentT(self.df, self.loc, self.scale).log_prob(torch.tensor(values)).sum() / self.Temperature\n",
    "\n",
    "    def sample(self,n):\n",
    "        return StudentT(self.df, self.loc, self.scale).sample((n,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Neural Network (Architecture: Fortuin et al. (2021))\n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, in_features = 28*28, out_features = 10, hidden_units = 100, hidden_layers = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input to first layer\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(in_features, hidden_units))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_units, hidden_units))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_units, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,28*28)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        class_probs = self.output_layer(x)\n",
    "        return class_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network (Architecture: Fortuin et al. (2021))\n",
    "\n",
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNN, self).__init__()\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add channel dimension to input tensor\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        # Apply ReLU non-linearity and max pooling after the first convolutional layer\n",
    "        x = F.relu(F.max_pool2d(F.relu(self.conv1(x)), 2))\n",
    "        # Apply ReLU non-linearity and max pooling after the second convolutional layer\n",
    "        x = F.relu(F.max_pool2d(F.relu(self.conv2(x)), 2))\n",
    "        # Flatten the output of the second convolutional layer\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        # Apply ReLU non-linearity to the output of the fully connected layer\n",
    "        class_probs = self.fc1(x)\n",
    "        return class_probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer SGLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/ratschlab/bnn_priors/blob/main/bnn_priors/mcmc/sgld.py\n",
    "\n",
    "def dot(a, b):\n",
    "    \"return (a*b).sum().item()\"\n",
    "    return (a.view(-1) @ b.view(-1)).item()\n",
    "\n",
    "\n",
    "class Fortuin_SGLD(torch.optim.Optimizer):\n",
    "    \"\"\"SGLD with momentum, preconditioning and diagnostics from Wenzel et al. 2020.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate\n",
    "        num_data (int): the number of data points in this learning task\n",
    "        momentum (float): momentum factor (default: 0)\n",
    "        temperature (float): Temperature for tempering the posterior.\n",
    "                             temperature=0 corresponds to SGD with momentum.\n",
    "        rmsprop_alpha: decay for the moving average of the squared gradients\n",
    "        rmsprop_eps: the regularizer parameter for the RMSProp update\n",
    "        raise_on_no_grad (bool): whether to complain if a parameter does not\n",
    "                                 have a gradient\n",
    "        raise_on_nan: whether to complain if a gradient is not all finite.\n",
    "    \"\"\"\n",
    "    def __init__(self, params: Sequence[Union[torch.nn.Parameter, Dict]], lr: float,\n",
    "                 num_data: int, momentum: float=0, temperature: float=1.,\n",
    "                 rmsprop_alpha: float=0.99, rmsprop_eps: float=1e-8,  # Wenzel et al. use 1e-7\n",
    "                 raise_on_no_grad: bool=True, raise_on_nan: bool=False):\n",
    "        assert lr >= 0 and num_data >= 0 and momentum >= 0 and temperature >= 0\n",
    "        defaults = dict(lr=lr, num_data=num_data, momentum=momentum,\n",
    "                        rmsprop_alpha=rmsprop_alpha, rmsprop_eps=rmsprop_eps,\n",
    "                        temperature=temperature)\n",
    "        super(Fortuin_SGLD, self).__init__(params, defaults)\n",
    "        self.raise_on_no_grad = raise_on_no_grad\n",
    "        self.raise_on_nan = raise_on_nan\n",
    "        # OK to call this one, but not `sample_momentum`, because\n",
    "        # `update_preconditioner` uses no random numbers.\n",
    "        self.update_preconditioner()\n",
    "        self._step_count = 0  # keep the `torch.optim.scheduler` happy\n",
    "\n",
    "    def _preconditioner_default(self, state, p) -> float:\n",
    "        try:\n",
    "            return state['preconditioner']\n",
    "        except KeyError:\n",
    "            v = state['preconditioner'] = 1.\n",
    "            return v\n",
    "\n",
    "    def delta_energy(self, a, b) -> float:\n",
    "        return math.inf\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_momentum(self, keep=0.0):\n",
    "        \"Sample the momenta for all the parameters\"\n",
    "        assert 0 <= keep and keep <= 1.\n",
    "        if keep == 1.:\n",
    "            return\n",
    "        for group in self.param_groups:\n",
    "            std = math.sqrt(group['temperature']*(1-keep))\n",
    "            for p in group['params']:\n",
    "                if keep == 0.0:\n",
    "                    self.state[p]['momentum_buffer'] = torch.randn_like(p).mul_(std)\n",
    "                else:\n",
    "                    self.state[p]['momentum_buffer'].mul_(math.sqrt(keep)).add_(torch.randn_like(p), alpha=std)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Optional[Callable[..., torch.Tensor]]=None,\n",
    "             calc_metrics=True, save_state=False):\n",
    "        assert save_state is False\n",
    "        return self._step_internal(self._update_group_fn, self._step_fn,\n",
    "                                   closure, calc_metrics=calc_metrics)\n",
    "    initial_step = step\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def final_step(self, closure: Optional[Callable[..., torch.Tensor]]=None,\n",
    "                   calc_metrics=True, save_state=False):\n",
    "        assert save_state is False\n",
    "        return self._step_internal(self._update_group_fn, self._step_fn,\n",
    "                                   closure, calc_metrics=calc_metrics,\n",
    "                                   is_final=True)\n",
    "\n",
    "\n",
    "    def _step_internal(self, update_group_fn, step_fn, closure, **step_fn_kwargs):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        try:\n",
    "            for group in self.param_groups:\n",
    "                update_group_fn(group)\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None:\n",
    "                        if self.raise_on_no_grad:\n",
    "                            raise RuntimeError(\n",
    "                                f\"No gradient for parameter with shape {p.shape}\")\n",
    "                        continue\n",
    "                    if self.raise_on_nan and not torch.isfinite(p.grad).all():\n",
    "                        raise ValueError(\n",
    "                            f\"Gradient of shape {p.shape} is not finite: {p.grad}\")\n",
    "                    step_fn(group, p, self.state[p], **step_fn_kwargs)\n",
    "\n",
    "        except KeyError as e:\n",
    "            if e.args[0] == \"momentum_buffer\":\n",
    "                raise RuntimeError(\"No 'momentum_buffer' stored in state. \"\n",
    "                                   \"Perhaps you forgot to call `sample_momentum`?\")\n",
    "            raise e\n",
    "        return loss\n",
    "\n",
    "    def _update_group_fn(self, g):\n",
    "        g['hn'] = math.sqrt(g['lr'] * g['num_data'])\n",
    "        g['h'] = math.sqrt(g['lr'] / g['num_data'])\n",
    "        g['noise_std'] = math.sqrt(2*(1 - g['momentum']) * g['temperature'])\n",
    "\n",
    "    def _step_fn(self, group, p, state, calc_metrics=True, is_final=False):\n",
    "        \"\"\"if is_final, do not change parameters or momentum\"\"\"\n",
    "        M_rsqrt = self._preconditioner_default(state, p)\n",
    "        d = p.numel()\n",
    "\n",
    "        # Update the momentum with the gradient\n",
    "        if group['momentum'] > 0:\n",
    "            momentum = state['momentum_buffer']\n",
    "            if calc_metrics:\n",
    "                # NOTE: the momentum is from the previous time step\n",
    "                state['est_temperature'] = dot(momentum, momentum) / d\n",
    "            if not is_final:\n",
    "                momentum.mul_(group['momentum']).add_(p.grad, alpha=-group['hn']*M_rsqrt)\n",
    "        else:\n",
    "            if not is_final:\n",
    "                momentum = p.grad.detach().mul(-group['hn']*M_rsqrt)\n",
    "            if calc_metrics:\n",
    "                # TODO: make the momentum be from the previous time step\n",
    "                state['est_temperature'] = dot(momentum, momentum) / d\n",
    "\n",
    "        if not is_final:\n",
    "            # Add noise to momentum\n",
    "            if group['temperature'] > 0:\n",
    "                momentum.add_(torch.randn_like(momentum), alpha=group['noise_std'])\n",
    "\n",
    "        if calc_metrics:\n",
    "            # NOTE: p and p.grad are from the same time step\n",
    "            state['est_config_temp'] = dot(p, p.grad) * (group['num_data']/d)\n",
    "\n",
    "        if not is_final:\n",
    "            # Take the gradient step\n",
    "            p.add_(momentum, alpha=group['h']*M_rsqrt)\n",
    "\n",
    "            # RMSProp moving average\n",
    "            alpha = group['rmsprop_alpha']\n",
    "            state['square_avg'].mul_(alpha).addcmul_(p.grad, p.grad, value=1 - alpha)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_preconditioner(self):\n",
    "        \"\"\"Updates the preconditioner for each parameter `state['preconditioner']` using\n",
    "        the estimated `state['square_avg']`.\n",
    "        \"\"\"\n",
    "        precond = OrderedDict()\n",
    "        min_s = math.inf\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            eps = group['rmsprop_eps']\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                try:\n",
    "                    square_avg = state['square_avg']\n",
    "                except KeyError:\n",
    "                    square_avg = state['square_avg'] = torch.ones_like(p)\n",
    "\n",
    "                precond[p] = square_avg.mean().item() + eps\n",
    "                min_s = min(min_s, precond[p])\n",
    "\n",
    "        for p, new_M in precond.items():\n",
    "            # ^(1/2) to form the preconditioner,\n",
    "            # ^(-1/2) because we want the preconditioner's inverse square root.\n",
    "            self.state[p]['preconditioner'] = (new_M / min_s)**(-1/4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Neural Network by MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNN:\n",
    "    def __init__(self, dataset_train, network, prior,\n",
    "     num_epochs = 300, max_size = 100, burn_in = 100, lr = 1e-3, sample_interval = 1):\n",
    "\n",
    "        # Hyperparameters and general parameters\n",
    "        self.learning_rate = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.burn_in = burn_in\n",
    "        self.sample_interval = sample_interval\n",
    "        self.max_size = max_size\n",
    "\n",
    "        self.batch_size = 128\n",
    "        self.print_interval = 50\n",
    "        \n",
    "        # Data Loader\n",
    "        self.data_loader = DataLoader(dataset_train, batch_size=self.batch_size, shuffle=True)\n",
    "        self.sample_size = dataset_train.__len__()\n",
    "\n",
    "        # Set Prior\n",
    "        self.prior = prior\n",
    "\n",
    "        # Initialize the network\n",
    "        self.network = network\n",
    "\n",
    "        # Set optimizer\n",
    "        self.optimizer = Fortuin_SGLD(self.network.parameters(), lr=self.learning_rate, num_data=self.batch_size)\n",
    "\n",
    "        # Scheduler for polynomially decreasing learning rates\n",
    "        self.scheduler = PolynomialLR(self.optimizer, total_iters = self.num_epochs, power = 0.5)\n",
    "\n",
    "        # Deque to store model samples\n",
    "        self.model_sequence = deque()\n",
    "\n",
    "    def train(self):\n",
    "        num_iter = 0\n",
    "        print('Training Modelihno')\n",
    "\n",
    "        self.network.train()\n",
    "        progress_bar = trange(self.num_epochs)\n",
    "\n",
    "        N = self.sample_size\n",
    "\n",
    "        for _ in progress_bar:\n",
    "            num_iter += 1\n",
    "\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(self.data_loader):\n",
    "                self.network.zero_grad()\n",
    "                n = len(batch_x)\n",
    "\n",
    "                # Perform forward pass\n",
    "                current_logits = self.network(batch_x)\n",
    "\n",
    "                # Calculate log_likelihood of weights for a given prior\n",
    "\n",
    "                parameters = self.network.state_dict()     # extract weights from network\n",
    "                param_values = list(parameters.values())    # list weights\n",
    "                param_flat = np.concatenate([v.flatten() for v in param_values])    # flattern\n",
    "                log_prior = self.prior.log_likelihood(param_flat)              # calculate log_lik\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = N/n*F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y) - log_prior#/#len(param_flat)\n",
    "\n",
    "                # Backpropagate to get the gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update Metrics according to print_interval\n",
    "                if batch_idx % self.print_interval == 0:\n",
    "                    current_logits = self.network(batch_x)\n",
    "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
    "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item(),\n",
    "                    nll_loss=N/n*F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y).item(),\n",
    "                    log_prior_normalized = - log_prior.item()/len(param_flat),\n",
    "                    lr = self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # Decrease lr based on scheduler\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Save the model samples if past the burn-in epochs according to sampling interval\n",
    "            if num_iter > self.burn_in and num_iter % self.sample_interval == 0:\n",
    "                self.model_sequence.append(copy.deepcopy(self.network))\n",
    "                # self.network.state_dict()\n",
    "\n",
    "            # If model_sequence to big, delete oldest model\n",
    "            if len(self.model_sequence) > self.max_size:\n",
    "                self.model_sequence.popleft()\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.network.eval()\n",
    "\n",
    "        # Sum predictions from all models in model_sequence\n",
    "        estimated_probability = torch.zeros((len(x), 10))\n",
    "\n",
    "        for model in self.model_sequence:\n",
    "\n",
    "            self.network.load_state_dict(model.state_dict())\n",
    "            logits = self.network(x).detach()\n",
    "            estimated_probability += F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Normalize the combined predictions to get average predictions\n",
    "        estimated_probability /= len(self.model_sequence)\n",
    "\n",
    "        assert estimated_probability.shape == (x.shape[0], 10)  \n",
    "        return estimated_probability\n",
    "    \n",
    "    def test(self,x):\n",
    "        # test set\n",
    "        x_test = x.data.float() \n",
    "        y_test = x.targets         \n",
    "\n",
    "        # predicted probabilities\n",
    "        class_probs = self.predict_probabilities(x_test)\n",
    "\n",
    "        # accuracy\n",
    "        accuracy = (class_probs.argmax(axis=1) == y_test).float().mean()\n",
    "        return  accuracy #print(f'Test Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "    def calibration(self,x):\n",
    "        # test set\n",
    "        x_test = x.data.float() \n",
    "        y_test = x.targets         \n",
    "\n",
    "        # predicted probabilities\n",
    "        class_probs = self.predict_probabilities(x_test)\n",
    "\n",
    "        calib_err = calibration_error(class_probs, y_test, n_bins = 30, task = \"multiclass\", norm=\"l1\", num_classes=10)\n",
    "        return calib_err #print(f'Calibration Error: {calib_err.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Networks = [FullyConnectedNN(), ConvolutionalNN()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Priors = [IsotropicGaussian(), StudentTPrior()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperatures = [1.0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracies = []\n",
    "Calibrations = []\n",
    "\n",
    "for network in Networks:\n",
    "    for prior in Priors:\n",
    "        for temperature in Temperatures:\n",
    "            lol = BayesianNN(sub_train_set,network = network, prior=prior, num_epochs=200)\n",
    "            lol.train()\n",
    "            Accuracies.append(lol.test(test_set).item())\n",
    "            Calibrations.append(lol.calibration(test_set).item())\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calibrations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f2fa85e4cfd485d54f024c1b8a04817ed39643e0edc5d72a4881a916b82e72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
